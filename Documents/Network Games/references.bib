@article{Bayer2019,
  abstract        = {This paper studies adaptive learning in the class of weighted network games. This class of games includes applications like research and development within interlinked firms, crime within social networks, the economics of pollution, and defense expenditures within allied nations. We show that for every weighted network game, the set of pure Nash equilibria is non-empty and, generically, finite. Pairs of players are shown to have jointly profitable deviations from interior Nash equilibria. If all interaction weights are either non-negative or non-positive, then Nash equilibria are Pareto inefficient. We show that quite general learning processes converge to a Nash equilibrium of a weighted network game if every player updates with some regularity.},
  author          = {Bayer, P{\'{e}}ter and Herings, P. Jean Jacques and Peeters, Ronald and Thuijsman, Frank},
  doi             = {10.1016/j.jedc.2019.06.004},
  file            = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bayer et al. - 2019 - Adaptive learning in weighted network games.pdf:pdf},
  issn            = {01651889},
  journal         = {Journal of Economic Dynamics and Control},
  keywords        = {Learning,Networks,Potential games,Public goods},
  mendeley-groups = {STAI/Network Games},
  month           = {aug},
  pages           = {250--264},
  publisher       = {Elsevier B.V.},
  title           = {{Adaptive learning in weighted network games}},
  volume          = {105},
  year            = {2019}
}
@techreport{Nagarajan2018,
abstract = {We study the asymptotic behavior of replicator dynamics in settings of network interaction. We focus on three agent graphical games where each edge/game is either a 2x2 zero-sum or a 2x2 coordination game. Using tools from dynamical systems such as Lyapunov functions and invariant functions we establish that this simple family of games can exhibit an interesting range of behaviors such as global convergence, periodicity for all initial conditions as well as limit cycles. In contrast, we do not observe more complex behavior such as toroids or chaos while it is possible to reproduce them in slightly more complicated settings.},
author = {Nagarajan, Sai Ganesh and Mohamed, Sameh and Piliouras, Georgios},
booktitle = {IFAAMAS},
file = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagarajan, Mohamed, Piliouras - 2018 - Three Body Problems in Evolutionary Game Dynamics Convergence, Periodicity and Limit Cycles.pdf:pdf},
keywords = {Cooperative games: theory and analysis,Evolutionary Games,Multi-agent learning,Noncooperative games: theory and analysis},
mendeley-groups = {STAI/Literature Review/MARL},
title = {{Three Body Problems in Evolutionary Game Dynamics: Convergence, Periodicity and Limit Cycles}},
url = {www.ifaamas.org},
volume = {9},
year = {2018}
}


@techreport{Bianchi,
  abstract        = {We consider a group of (multi)-integrator agents playing games on a network, in a partial-decision information scenario. We design fully distributed continuous-time controllers, based on consensus and primal-dual gradient dynamics, to drive the agents to a generalized Nash equilibrium. Our first solution adopts fixed gains, whose choice requires the knowledge of some global parameters of the game. Therefore, to adapt the procedure to setups where the agents do not have any global information, we introduce a controller that can be tuned in a completely decentralized fashion, thanks to the use of integral adaptive weights. We further introduce algorithms, both with constant and dynamic gains, specifically devised for generalized aggregative games. For all the proposed control schemes, we show convergence to a variational equilibrium, under Lipschitz continuity and strong monotonicity of the game mapping, by leveraging monotonicity properties and stability theory for projected dynamical systems.},
  archiveprefix   = {arXiv},
  arxivid         = {1911.12266v1},
  author          = {Bianchi, Mattia and Grammatico, Sergio},
  eprint          = {1911.12266v1},
  file            = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bianchi, Grammatico - Unknown - Continuous-time fully distributed generalized Nash equilibrium seeking for multi-integrator agents.pdf:pdf},
  mendeley-groups = {STAI/Network Games},
  title           = {{Continuous-time fully distributed generalized Nash equilibrium seeking for multi-integrator agents}}
}
@article{Bianchi2019,
  abstract        = {We consider a system of single- or double integrator agents playing a generalized Nash game over a network, in a partial-information scenario. We address the generalized Nash equilibrium seeking problem by designing a fully-distributed dynamic controller, based on continuous-time consensus and primal-dual gradient dynamics. Our main technical contribution is to show convergence of the closed-loop system to a variational equilibrium, under strong monotonicity and Lipschitz continuity of the game mapping, by leveraging monotonicity properties and stability theory for projected dynamical systems.},
  archiveprefix   = {arXiv},
  arxivid         = {1910.11608},
  author          = {Bianchi, Mattia and Grammatico, Sergio},
  eprint          = {1910.11608},
  file            = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bianchi, Grammatico - 2019 - A continuous-time distributed generalized Nash equilibrium seeking algorithm over networks for double-integ.pdf:pdf},
  mendeley-groups = {STAI/Network Games},
  month           = {oct},
  title           = {{A continuous-time distributed generalized Nash equilibrium seeking algorithm over networks for double-integrator agents}},
  url             = {http://arxiv.org/abs/1910.11608},
  year            = {2019}
}
@article{DePersis2020,
  abstract        = {We consider continuous-time equilibrium seeking in a class of aggregative games with strongly convex cost functions and affine coupling constraints. We propose simple semidecentralized integral dynamics and prove their global asymptotic convergence to a variational generalized aggregative or Nash equilibrium. The proof is based on Lyapunov arguments and invariance techniques for differential inclusions.},
  author          = {{De Persis}, Claudio and Grammatico, Sergio},
  doi             = {10.1109/TAC.2019.2939639},
  issn            = {15582523},
  journal         = {IEEE Transactions on Automatic Control},
  keywords        = {Aggregative game theory,decentralized control,multiagent systems,projected dynamical systems},
  mendeley-groups = {STAI/Network Games},
  month           = {may},
  number          = {5},
  pages           = {2171--2176},
  publisher       = {Institute of Electrical and Electronics Engineers Inc.},
  title           = {{Continuous-Time Integral Dynamics for a Class of Aggregative Games with Coupling Constraints}},
  volume          = {65},
  year            = {2020}
}
@article{Ewerhart2019,
  abstract        = {This paper studies {\ldots}ctitious play in networks of noncooperative two-4 person games. We show that continuous-time {\ldots}ctitious play converges to the set 5 of Nash equilibria if the overall n-person game is zero-sum. Moreover, the rate of 6 convergence is 1== , regardless of the size of the network. In contrast, arbitrary n-7 person zero-sum games with bilinear payo¤ functions do not possess the continuous-8 time {\ldots}ctitious-play property. As extensions, we consider networks in which each 9 bilateral game is either strategically zero-sum, a weighted potential game, or a two-10 by-two game. In those cases, convergence requires a condition on bilateral payo¤s or, 11 alternatively, that the network is acyclic. Our results hold also for the discrete-time 12 variant of {\ldots}ctitious play, which implies, in particular, a generalization of Robinson's 13 theorem to arbitrary zero-sum networks. Applications include security games, con ‡ict 14 networks, and decentralized wireless channel selection. 15},
  author          = {Ewerhart, Christian and Valkanova, Kremena},
  file            = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ewerhart, Valkanova - 2019 - Fictitious Play in Networks Fictitious Play in Networks 1.pdf:pdf},
  issn            = {1664-7041},
  mendeley-groups = {STAI/Network Games},
  title           = {{Fictitious Play in Networks Fictitious Play in Networks* 1}},
  year            = {2019}
}
@article{Li2017,
  abstract        = {This paper develops an off-policy reinforcement learning (RL) algorithm to solve optimal synchronization of multiagent systems. This is accomplished by using the framework of graphical games. In contrast to traditional control protocols, which require complete knowledge of agent dynamics, the proposed off-policy RL algorithm is a model-free approach, in that it solves the optimal synchronization problem without knowing any knowledge of the agent dynamics. A prescribed control policy, called behavior policy, is applied to each agent to generate and collect data for learning. An off-policy Bellman equation is derived for each agent to learn the value function for the policy under evaluation, called target policy, and find an improved policy, simultaneously. Actor and critic neural networks along with least-square approach are employed to approximate target control policies and value functions using the data generated by applying prescribed behavior policies. Finally, an off-policy RL algorithm is presented that is implemented in real time and gives the approximate optimal control policy for each agent using only measured data. It is shown that the optimal distributed policies found by the proposed algorithm satisfy the global Nash equilibrium and synchronize all agents to the leader. Simulation results illustrate the effectiveness of the proposed method.},
  author          = {Li, Jinna and Modares, Hamidreza and Chai, Tianyou and Lewis, Frank L. and Xie, Lihua},
  doi             = {10.1109/TNNLS.2016.2609500},
  issn            = {21622388},
  journal         = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords        = {Graphical game,multiagent systems (MAS),neural network (NN),reinforcement learning (RL),synchronization},
  mendeley-groups = {STAI/Network Games},
  month           = {oct},
  number          = {10},
  pages           = {2434--2445},
  pmid            = {28436891},
  publisher       = {Institute of Electrical and Electronics Engineers Inc.},
  title           = {{Off-policy reinforcement learning for synchronization in multiagent graphical games}},
  volume          = {28},
  year            = {2017}
}
@article{Paccagnan2019,
  abstract        = {Large scale systems are forecasted to greatly impact our future lives thanks to their wide ranging applications including cooperative robotics, mobility on demand, resource allocation, supply chain management. While technological developments have paved the way for the realization of such futuristic systems, we have a limited grasp on how to coordinate the individual components to achieve the desired global objective. This thesis deals with the analysis and coordination of large scale systems without the need of a centralized authority. In the first part of this thesis, we consider non-cooperative decision making problems where each agent's objective is a function of the aggregate behavior of the population. First, we compare the performance of an equilibrium allocation with that of an optimal allocation and propose conditions under which all equilibrium allocations are efficient. Towards this goal, we prove a novel result bounding the distance between the strategies at a Nash and Wardrop equilibrium that might be of independent interest. Second, we show how to derive scalable algorithms that guide agents towards an equilibrium allocation. In the second part of this thesis, we consider large-scale cooperative problems, where a number of agents need to be allocated to a set of resources with the goal of jointly maximizing a given submodular or supermodular set function. Since this class of problems is computationally intractable, we aim at deriving tractable algorithms for attaining approximate solutions. We approach the problem from a game-theoretic perspective and ask the following: how should we design agents' utilities so that any equilibrium configuration is almost optimal? To answer this question we introduce a novel framework that allows to characterize and optimize the system performance as a function of the chosen utilities by means of a tractable linear program.},
  archiveprefix   = {arXiv},
  arxivid         = {1901.06287},
  author          = {Paccagnan, Dario},
  doi             = {10.3929/ethz-b-000314981},
  eprint          = {1901.06287},
  file            = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paccagnan - 2019 - Distributed control and game design From strategic agents to programmable machines.pdf:pdf},
  keywords        = {Algorithms,Combinatorial Optimization,Game Theory,Multiagent Systems},
  mendeley-groups = {STAI/Network Games},
  month           = {jan},
  publisher       = {ETH Zurich},
  title           = {{Distributed control and game design: From strategic agents to programmable machines}},
  url             = {http://arxiv.org/abs/1901.06287 http://dx.doi.org/10.3929/ethz-b-000314981},
  year            = {2019}
}
@inproceedings{Parise,
  abstract        = {"The IEEE CDC is hosted by the IEEE Control Systems Society (CSS)"--Homepage der Konferenz Literaturangaben The CDC is recognized as the premier scientific and engineering conference dedicated to the advancement of the theory and practice of systems and control The CDC annually brings together an international community of researchers and practitioners in the field of automatic control to discuss new research results, perspectives on future developments, and innovative applications relevant to decision making, systems and control, and related areas},
  author          = {Parise, Francesca and Gentile, Basilio and Grammatico, Sergio and Lygeros, John},
  file            = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parise et al. - Unknown - Network aggregative games Distributed convergence to Nash equilibria.pdf:pdf},
  isbn            = {9781479978861},
  mendeley-groups = {STAI/Network Games},
  title           = {{Network aggregative games: Distributed convergence to Nash equilibria}}
}
@inproceedings{Talebi2019,
  abstract        = {In this paper, we propose a distributed no-regret learning algorithm for network games using a primal-dual method, i.e., dual averaging. With only locally available observations, we consider the scenario where each player optimizes a global objective, formed by local objective functions on the nodes of a given communication graph. Our learning algorithm for each player involves taking steps along their individual payoff gradients, dictated by local observations of the other player's actions. The output is then projected back-again locally-to the set of admissible actions for each player. We provide the regret analysis of this distributed learning algorithm for the case of a deterministic network that is subjected to two teams with distinct objectives, and obtain an O($\backslash$sqrt T $\backslash$log (T)) regret bound. Our analysis indicates the key correlation between the rate of convergence and network connectivity that also appears in the distributed optimization setup via dual averaging. Furthermore, we show that the point of convergence of the proposed algorithm would be a Nash Equilibrium of the game. Finally, we showcase by an illustrative example the performance of our algorithm in relation to the size and connectivity of the network.},
  author          = {Talebi, Shahriar and Alemzadeh, Siavash and Ratliff, Lillian J. and Mesbahi, Mehran},
  booktitle       = {Proceedings of the IEEE Conference on Decision and Control},
  doi             = {10.1109/CDC40024.2019.9030002},
  file            = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Talebi et al. - 2019 - Distributed Learning in Network Games A Dual Averaging Approach.pdf:pdf},
  isbn            = {9781728113982},
  issn            = {07431546},
  mendeley-groups = {STAI/Network Games},
  month           = {dec},
  pages           = {5544--5549},
  publisher       = {Institute of Electrical and Electronics Engineers Inc.},
  title           = {{Distributed Learning in Network Games: A Dual Averaging Approach}},
  volume          = {2019-December},
  year            = {2019}
}
@article{Tatarenko2020,
  abstract        = {We address online bandit learning of Nash equilibria in multi-agent convex games. We propose an algorithm whereby each agent uses only obtained values of her cost function at each joint played action, lacking any information of the functional form of her cost or other agents' costs or strategies. In contrast to past work where convergent algorithms required strong monotonicity, we prove that the algorithm converges to a Nash equilibrium under mere monotonicity assumption. The proposed algorithm extends the applicability of bandit learning in several games including zero-sum convex games with possibly unbounded action spaces, mixed extension of finite-action zero-sum games, as well as convex games with linear coupling constraints.},
  archiveprefix   = {arXiv},
  arxivid         = {2009.04258},
  author          = {Tatarenko, Tatiana and Kamgarpour, Maryam},
  eprint          = {2009.04258},
  file            = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tatarenko, Kamgarpour - 2020 - Bandit Online Learning of Nash Equilibria in Monotone Games.pdf:pdf},
  mendeley-groups = {STAI/Network Games},
  title           = {{Bandit Online Learning of Nash Equilibria in Monotone Games}},
  year            = {2020}
}
@article{Vamvoudakis2017,
  abstract        = {In this paper, we consider the problem of leader synchronization in systems with interacting agents in large networks while simultaneously satisfying energy-related user-defined distributed optimization criteria. But modeling in large networks is very difficult, and for that reason, we derive a model-free formulation that is based on a separate distributed Q-learning function for every agent. Every Q-function is a parametrization of each agent's control, of the neighborhood controls, and of the neighborhood tracking error. It is also evident that none of the agents has any information on where the leader is connected to and from where she spreads the desired information. The proposed algorithm uses an integral reinforcement learning approach with a separate distributed actor/critic network for each agent: a critic approximator to approximate each value function and an actor approximator to approximate each optimal control law. The derived tuning laws for each actor and critic approximators are designed appropriately by using gradient descent laws. We provide rigorous stability and convergence proofs to show that the closed-loop system has an asymptotically stable equilibrium point and that the control policies form a graphical Nash equilibrium. We demonstrate the effectiveness of the proposed method on a network consisting of 10 agents. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
  author          = {Vamvoudakis, Kyriakos G.},
  doi             = {10.1002/rnc.3719},
  issn            = {10498923},
  journal         = {International Journal of Robust and Nonlinear Control},
  keywords        = {Q-learning,actor/critic structures,games on graphs,large complex networks},
  mendeley-groups = {STAI/Network Games},
  month           = {nov},
  number          = {16},
  pages           = {2900--2920},
  publisher       = {John Wiley and Sons Ltd},
  title           = {{Q-learning for continuous-time graphical games on large networks with completely unknown linear system dynamics}},
  url             = {http://doi.wiley.com/10.1002/rnc.3719},
  volume          = {27},
  year            = {2017}
}
