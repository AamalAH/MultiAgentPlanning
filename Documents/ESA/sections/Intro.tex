\documentclass[.../main.tex]{subfiles}

\begin{document}

    Swarm robotics bases its success upon the collaboration of many
    agents who contribute towards the overall goal of the system
    \cite{Hamann2018}. Such a collaboration can take the form of cooperative efforts or even
    competition, both resulting in behaviours that are far more complex thhan individual robots can
    achieve. The capacity for complexity is
    becoming increasingly important as robots are presented
    with ever more challenging tasks including: search and rescue,
    cleaning up space debris, and robotic assembly. Such tasks are
    beyond the capabilities of individual robots and so we must
    leverage the capacity for agents in a swarm to collectively
    achieve our goals. Fortunately, a growing subset of the Control
    Theory community has turned their attention towards this very
    problem and address the question: how does the dynamical behaviour
    of a population of agents change under the influence of control?
    Note that we use the term `agent' loosely here, an agent can refer
    to anything from a simple particle \cite{Roy2017} to a complex
    robot \cite{Elamvazhuthi2019}. In answering this question,
    researchers have gained a stronger understanding of how a swarm
    may be controlled to achieve desired results.

    Another technique which has made incredible strides towards
    improving the capability of autonomous systems to communally
    achieve results is Multi-Agent Reinforcement Learning (MARL)
    \cite{SchwartzMulti-agentApproach}. This builds on the ideas of
    classical Reinforcement Learning which allows an agent to optimise
    its strategies towards tackling a task through repeated exposure
    to that task.  MARL utilises this same principle with the caveat
    that agents must adapt their strategies based not only on the
    outcome of the action, but also on the actions of the other
    agents. This has shown strong results in practical applications
    \cite{Woolridge2009, SchwartzMulti-agentApproach,
      Yang2004}. Whilst MARL was once considered to be
    non-identifiable (there is no clear way of identifying the
    algorithm that a particular agent was trained on) and black box
    (there is no clear way of ensuring a desired result from a
    learning algorithm), a steady stream of research is emerging to
    lift this fog \cite{Bloembergen2015}. This allows for researchers
    to choose the parameters of their learning algorithms
    appropriately to ensure that learning agents behave in the desired
    manner.

    \section{Problem Statement} \label{sec::Problem_Statement}

    Based on the successes of the swarm control and MARL communities,
    a natural question to ask is whether the promises of these
    disciplines may be unified. Such a combination would radically
    improve the capabilities of autonomous systems. Ultimately such a
    system must be driven under the influence of control inputs to
    perform given tasks. Examples of this might be: forming around
    individual components of space debris and moving them to a desired
    location (e.g. a bin), or relocating to multiple areas in a
    disaster zone to provide immediate relief. For such tasks to be
    achieved in dynamic environments, which is typically the case, the
    system must be able to constantly adapt its approach and respond
    accordingly. Given these considerations, the problem may be stated
    as follows

    \begin{enumerate}
    	\item A population of robots is given a goal which must be achieved, and can often be
    	deconstructed into a set of smaller tasks, to be executed simultaneously. 
    	\item These are to be performed in a dynamic environment with obstacles present and where
    	the number of tasks, and their priorities may be in constant flux.
    	\item The agents in the system must, therefore, make decisions collectively which will result
        in the system achieving the given goals. Due to the dynamic quality of the problem, these
        decisions may need to be evaluated online and in a decentralised fashion, as it may not be
        possible for a centralised decision maker to communicate with all agents. 
        \item It is desirable, therefore, to control the dynamical behaviour of
    	a system composed of agents who learn and adapt through interactions with each other. In
    	addition, to ensure the safe operation of the system, properties such as state and control
    	constraints must be satisfied.
    \end{enumerate}


    With the goal of addressing this problem in mind, the aim of this PhD is to address the question
    posed by the final point: how does the dynamical behaviour of a swarm of intelligent, adaptive
    agents evolve under the influence of defined control inputs?

    \section{Objectives and Scope} \label{sec::Objectives_and_Scope}

    In particular, we will study

    \begin{itemize}
    	\item How the strategies of a population of agents evolves as they iteratively interact
    	with one another. We will examine whether the system evolves to a stable equilibrium, or
    	whether it exhibits complex, even chaotic behaviour.
    	\item The extent to which the dynamical behaviour of a population of learning agents is
    	influenced under given controls. We seek to understand the conditions under which guarantees
    	on theoretical results such as controllability, stability, and well-posedness may be
    	established.
    \end{itemize}

    The resultant work will provide novel insights into the behaviour of swarm systems and provide
    new methods for which they may be practically used. 

    Importantly, the study will seek to understand the guarantees that can be
    placed on the safety of such systems in terms of their stability and
    constraint satisfaction. In doing so, we can ensure that a user can have
    confidence in the decision making and manipulation of the system.

\end{document}
