\documentclass[.../main.tex]{subfiles}

\begin{document}

    In this chapter, we summarise and present the research and results which have been obtained thus
    far. These tackle the problems described in Section \ref{sec::Chaos_in_MARL}. We detail the
    derivation of a stability phase line as presented in Sanders et al. \cite{Sanders2018} as well
    as the numerical simulations performed to verify these results. The steps to be completed
    subseqent to this study are then given.

    \section{Convergence and Chaos in Q-Learning Agents} \label{sec::Chaos_in_Q-Learning}

    We begin by considering the evolution of learning with agents who follow a Q-Learning approach.
    Specifically, we look at characterising the stability of agents learning strategies. This
    technique was first carried out by Sanders et al \cite{Sanders}, in which the authors
    characterised the strategy evolution of agents who learn using an 'Experience Weighted
    Attraction' (EWA) algorithm, which is seen to be a strong representation of how people learn in
    games. The authors were able to determine the regions of parameter space in which learning is
    likely to converge towards stable equilibriums, as opposed to complex behaviours (such as limit
    cycles or chaotic dynamics). The aim of this study is to format these techniques for the study
    of computational agents who follow the popular Q-Learning approach \cite{SuttonBarto,Schwarz}.
    {}
    We aim, similarly, to determine the regions of parameter space in which the agents are likely to
    converge to stable equilibria. In this way it will be possible to, a priori, determine whether,
    for a particular game, the behaviour is likely to converge and even how to choose the agent or
    game parameters to ensure the predicatability of the resulting behaviour. To achieve this, we
    consult the Q-Learning Dynamics proposed by Tuyls et al \cite{TuylsDynamics}. In this study, the
    authors were able to derive a continuous time dynamical system describing how agents following a
    Q-Learning approach adjust the probabilities of choosing actions as they iteratively play a
    game. These equations are shown in \cite{TuylsDynamics} to accurately model the expected
    behaviour of agents as they iterate a game, and the experiments which verified this model are
    shown in \textbf{Figures ...} . 

    With the accuracy of the continuous time dynamical system (\ref{eqn::EOM}) established, we analyse the
    stability of this system in the following section. Note that whilst the derivation provided is
    for the particular case of two agents for the sake of brevity, the problem of p-player games is
    equivalent to that of two players and so the solutions to the former can (and will) be
    presented. 

     \textbf{Derivation suppressed, uncomment to see}
     %\subfile{./sections/ResearchSummary.tex} 
    
    \subsection*{Future Work} \label{sec::Future Work}

    As mentioned, the above system considers only a two player game. This was done with the intention
    of simplifying the notation and reducing the complexity of the experiments. However, the next
    immediate action is to present the equivalent solutions for the case of a general p-player game.
    We then seek to expand this study to a population of agents. For this, we will study the
    stability of the system presented by Leung et al \cite{Hu2019} which presents a mean-field model
    describing the evolution of learning of a population of Q-Learning agents who play co-operative
    games against one another. This last point is an important limitation - the derived dynamics
    will only consider populations who must co-operate with one another (i.e. the $\Gamma$ value of
    the game is one). 

\end{document}