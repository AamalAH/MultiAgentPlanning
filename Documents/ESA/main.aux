\relax 
\pgfsyspdfmark {pgfid1}{4903155}{47589214}
\citation{Hamann2018}
\citation{Roy2017}
\citation{Elamvazhuthi2019}
\citation{Sahin2005}
\citation{SchwartzMulti-agentApproach}
\citation{Woolridge2009}
\citation{SchwartzMulti-agentApproach}
\citation{Yang2004}
\citation{Bloembergen2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Statement}{2}}
\newlabel{sec::Problem_Statement}{{1.1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Objectives and Scope}{3}}
\newlabel{sec::Objectives_and_Scope}{{1.2}{3}}
\@writefile{toc}{\contentsline {paragraph}{Relevance to Safe and Trusted AI}{3}}
\newlabel{ch::Intro}{{1}{3}}
\citation{OliehoekDecentralizedPOMDPs}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Multi Agent Systems}{4}}
\newlabel{sec::Multi_Agent_Systems}{{2.1}{4}}
\citation{Rizk2018}
\citation{Ray2010}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Game Theory}{5}}
\newlabel{sec::Game_Theory}{{2.2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Partially Observed Stochatic Games (POSG)}{5}}
\newlabel{sec::Stochastic_Games}{{2.2.1}{5}}
\citation{Emery-Montemerlo}
\citation{Ray2010}
\citation{Dai2018}
\citation{Marden2018AnnualControl}
\citation{Breitner1994ReentryGame}
\citation{Bardi1991DifferentialDisturbances}
\citation{Mylvaganam2017AutonomousApproach}
\citation{MylvaganamASystems}
\citation{Mylvaganam2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Game Theoretic Control}{6}}
\newlabel{Game Theoretic Control}{{2.2.2}{6}}
\citation{Yang2004}
\citation{Zhang2019}
\citation{Hernandez-LealA}
\citation{SchwartzMulti-agentApproach}
\citation{SchwartzMulti-agentApproach}
\citation{Bowling2002}
\citation{SchwartzMulti-agentApproach}
\citation{ConitzerAWESOME:}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Multi Agent Reinforcement Learning}{7}}
\newlabel{sec::MARL}{{2.3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Learning in Two Player Games}{7}}
\newlabel{sec::Two_Player_Games}{{2.3.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Learning in Stochastic Games}{7}}
\newlabel{sec::Learning_Stochastic_Games}{{2.3.2}{7}}
\citation{Littman}
\citation{Hu}
\citation{Gleave}
\citation{ShohamMultiagentFoundations}
\citation{Letcher2019DifferentiableMechanics}
\citation{Tuyls2006AnGames}
\citation{Tuyls2006AnGames}
\citation{Bloembergen2015}
\citation{Tuyls2006AnGames}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Multi Agent Learning Dynamics}{8}}
\newlabel{sec::MARL_Dynamics}{{2.3.3}{8}}
\citation{Hu2019}
\citation{Imhof2005}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Control Theory}{9}}
\newlabel{sec::Control_Theory}{{2.4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}A Brief Introduction to Model Predictive Control}{9}}
\newlabel{sub:a_brief_introduction_to_model_predictive_control}{{2.4.1}{9}}
\citation{rawlings2017model}
\citation{Negenborn2014}
\citation{Venkat2006}
\citation{Liu2019}
\citation{VanParys2017}
\citation{Zheng2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Distributed Model Predictive Control}{10}}
\newlabel{sec::Distributed_MPC}{{2.4.2}{10}}
\citation{Foerster}
\citation{Heirung2019}
\citation{Couceiro2015}
\citation{Sethi2017}
\citation{Gao2018}
\citation{Rizk2018}
\citation{Sahin2005}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Swarms}{11}}
\newlabel{sec::Swarms}{{2.5}{11}}
\citation{Hamann2008}
\citation{Hamann2008}
\citation{Elamvazhuthi2019}
\citation{Elamvazhuthi2019b}
\citation{Li2017}
\citation{Inoue2019}
\citation{Inoue2019}
\citation{Bellomo2017}
\citation{Borzi2015}
\citation{Pini2011TaskSelection}
\citation{Zahadat2016DivisionInhibition}
\citation{Pini2011TaskSelection}
\citation{Couceiro2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Approaches to Swarm Control}{12}}
\newlabel{sec::Swarm Control}{{2.5.1}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Decision Making in Swarms}{12}}
\newlabel{sec::Decisions_in_Swarms}{{2.5.2}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Discussion}{13}}
\newlabel{sec:remarks}{{2.6}{13}}
\newlabel{ch::Lit_Review}{{2}{13}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Introduction to Proposals}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch::IntrotoProposals}{{3}{15}}
\citation{Galla2011}
\citation{Sato2002}
\citation{Strogatz2000}
\citation{Fradkov2009}
\citation{Tuyls2006AnGames}
\citation{Sanders2018}
\citation{Tuyls2006AnGames}
\citation{Sanders2018}
\citation{Tuyls2006AnGames}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Stability and Chaos in MARL}{16}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces   Different types of dynamical behaviour displayed by learning agents. a) Figure drawn from \cite  {Tuyls2006AnGames}. Convergence to a unique fixed point in the upper right corner (1, 1). This fixed point is unique, in that all trajectories, regardless of initialisation, will converge to this point. b) Limit Cycle, the trajectories converge to cyclic behaviour c, d) Chaotic behaviour, here small deviatons in the initial conditions can grow exponentially. b-d drawn from \cite  {Sanders2018}\relax }}{16}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig::DynamicalBehaviours}{{4.1}{16}}
\citation{Tuyls2006AnGames}
\citation{Tuyls2006AnGames}
\citation{Sanders2018}
\citation{Galla2011}
\citation{Camerer2009}
\citation{Sanders2018}
\citation{Sanders2018}
\citation{Sanders2018}
\newlabel{eqn::EOM}{{4.1}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces   Figures taken from \cite  {Tuyls2006AnGames} a) Phase plot showing the expected behaviour of Q-Learning agents trained by iterating the Prisoner's Dilemma game as predicted by (\G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `eqn::EOM' on page 17 undefined}). From left to right, the agents have parameter $\tau = 1, 2, 10$. b) Corresponding trajectories of Q-Learning agents with randomised initial conditions displayed through numerical simulation. It is clear that the trajectories in b) follow the predictions in a), after stochasticity is accounted for. \relax }}{17}}
\newlabel{fig::TuylsExperiments}{{4.2}{17}}
\citation{Sanders2018}
\citation{Sanders2018}
\citation{Camerer1999}
\citation{Camerer2003}
\citation{Sutton2018}
\citation{SchwartzMulti-agentApproach}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces   Results as produced in the supplementary material of Sanders et al \cite  {Sanders2018}. Here, $p$ is the number of players, $N$ is the number of actions, $\Gamma $ represents the competitiveness of the game (-1 represents zero-sum, 1 represents shared rewards) and $\alpha $ is the memory parameter of the agent as before. Here $\tau $ is held constant at 0.05 (though it is referred to as $\beta $ in the paper). The black region represents where games converged to a fixed point through all simulations, whilst the hotter regions represent the more complex dynamics - red shows the presence of limit cycles and white regions signal the onset of chaos. The green line is a theoretical estimation for the phase line which separates the convergent dynamics from complex.\relax }}{18}}
\newlabel{fig::GallaPredictions}{{4.3}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Summary of Research}{18}}
\citation{Tuyls2006AnGames}
\citation{Tuyls2006AnGames}
\citation{Sanders2018}
\citation{Tuyls2006AnGames}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Convergence and Chaos in Q-Learning Agents}{19}}
\newlabel{sec::Chaos_in_Q-Learning}{{4.2}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Dynamics of Q-Learning}{19}}
\newlabel{sub:dynamics_of_q_learning}{{4.2.1}{19}}
\newlabel{eq::Q-Learning}{{4.2}{19}}
\newlabel{eq::actionselection}{{4.3}{19}}
\citation{Coolen2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Derivation of stability line}{20}}
\newlabel{sub:derivation_of_stability_line}{{4.2.2}{20}}
\newlabel{eqn::Transformed}{{4.7}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Numerical Experiments}{21}}
\newlabel{sub:numerical_experiments}{{4.2.3}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces $\alpha = 0.1$, $\gamma = 0.1$, $\tau \in [0.1, 10]$, $\Gamma \in [-1, 1]$. Each simulation is run for 1 $\times 10^5$ iterations and tested for convergence. The game is said to have converged based on a tolerance of 1\% difference between action probabilities. For each combination of $\tau $, $\Gamma $ the game is played 5 times, each with random payoff matrices and initial conditions. The average number of converged games (giving an indication of probability of convergence) is shown in each cell of the heatmap. \relax }}{21}}
\newlabel{fig::NumericalExperiments}{{4.4}{21}}
\citation{Hu2019}
\newlabel{sec::Future Work}{{4.2.3}{22}}
\newlabel{ch::ShortTermGoals}{{4}{22}}
\citation{Sanders2018}
\citation{Sanders2018}
\citation{Sanders2018}
\citation{Elamvazhuthi2019b}
\citation{Hu2019}
\citation{Hu2019}
\citation{Hu2019}
\citation{Doerr2019}
\citation{Nagarajan2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Proposals for Future Research}{23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Large Population Dynamics}{23}}
\newlabel{sec::Large_Agent_Dynamics}{{5.1}{23}}
\citation{Elamvazhuthi2019}
\citation{Li2017}
\citation{Roy2017}
\citation{Fleig}
\citation{Burger2019}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces   Results as produced in the supplementary material of Sanders et al \cite  {Sanders2018}. The same items appear here as in Figure \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig::GallaPredictions' on page 24 undefined}, except the value of p (number of players) increases moving from top left to bottom right. It is clear that the hotter region occupies a larger region of parameter space, indicating that as the bumber of players in the game increase, chaos becomes more prevalent.\relax }}{24}}
\newlabel{fig::GallaPrevalence}{{5.1}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Swarm Control through Fields}{24}}
\newlabel{sec::Swarm_Field_Control}{{5.2}{24}}
\citation{Bellomo2017}
\citation{Fredi2010}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces   Results as produced in Leung et al. \cite  {Hu2019}. Here, the solid and dotted lines represent the evolution of the expectation of Q-values of two actions across a population of 50 or 999 agents respectively who are trained on an iterated Stag Hunt game. The circled and crossed lines represent the dynamics of these Q-values as predicted by the system of equations derived in the paper. These are seen to closely match the results of the numerical experiments.\relax }}{25}}
\newlabel{fig::LeungPredictions}{{5.2}{25}}
\newlabel{eqn::Vlasov}{{5.1}{25}}
\citation{Bellomo2017}
\citation{Ko2019}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Incorporation of Intelligence in Control}{26}}
\newlabel{sec::Intelligence_in_control}{{5.3}{26}}
\newlabel{ch::ProposedResearch}{{5}{26}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Research Timeline}{27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{IEEEtran}
\bibdata{IEEEfull,references}
\newlabel{ch::ResearchTimeline}{{6}{28}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Research Summary}{30}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eqn::appEOM}{{A.1}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Rescaling of Variables}{30}}
\newlabel{sub:apprescaling_of_variables}{{A.1}{30}}
\newlabel{eqn::appscaledEOM}{{A.4}{30}}
\citation{Mezard1986}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Generating Functional}{31}}
\newlabel{sub:appgenerating_functional}{{A.2}{31}}
\newlabel{eqn::appgeneratingfunctional}{{A.6}{31}}
\newlabel{eqn::appmoveddisorder}{{A.7}{31}}
\citation{ZinnJustin2009}
\newlabel{eqn::appexpectationIdentity}{{A.10}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}Expectation of $Q_1$}{32}}
\newlabel{ssub:appexpectation_of_Q1}{{A.2.1}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.2}Expectation of $Q_2$}{33}}
\newlabel{ssub:appexpectation_of_Q2}{{A.2.2}{33}}
\citation{Opper1992}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Stability Analysis}{36}}
\newlabel{sec:appstability_analysis}{{A.3}{36}}
\newlabel{eqn::appLinearisation}{{A.32}{37}}
\newlabel{eqn::appTransformed}{{A.35}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Numerical Simulations}{38}}
\newlabel{sec:appnumerical_simulations}{{A.4}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces $\alpha = 0.1$, $\gamma = 0.1$, $\tau \in [0.1, 10]$, $\Gamma \in [-1, 1]$. Each simulation is run for 1e5 iterations and tested for convergence. The game is said to have converged based on a tolerance of 1\% difference between action probabilities. For each combination of $\tau $, $\Gamma $ the game is played 5 times, each with random payoff matrices and initial conditions. The average number of converged games (giving an indication of probability of convergence) is shown in each cell of the heatmap. \relax }}{38}}
\newlabel{fig::appToys}{{A.1}{38}}
\newlabel{app::ResearchSummary}{{A}{39}}
