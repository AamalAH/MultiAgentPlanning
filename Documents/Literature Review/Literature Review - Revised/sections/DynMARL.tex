\documentclass[../sample.tex]{subfiles}

\begin{document}
    The purpose of this chapter is to further develop the ideas presented in Chapter
    \ref{ch::Proposals}. As mentioned, there are four main areas which have been identified for
    development in this field. These all aim to better apply the study to cases which are more
    relevant for real world Reinforcement Learning problems. To this end, the following studies are
    proposed:
    
    \begin{itemize}
        \item Dynamics of large agent populations with heterogenous agents.
        \item Dynamics of Reinforcement Learning in continuous action spaces.
        \item Dynamics of Learning in 'stateful' environments.
        \item Characterisation of complex dynamics using Reinforcement Learning Algorithms
    \end{itemize}

    In the above, the term 'stateful' is borrowed from Bloembergen et al. \cite{Bloembergen2015} and
   refers to the consideration of state transitions in the game. The remainder of this chapter will
   address current research in the above topics and avenues for attempting new results.
   
   \section{Large Agent Dynamics}

    This suggestion aims to build upon the work presented by Leung et
    al. \cite{Hu2019} in which the point is made that the vast
    majority of the work done in this area considers games with a
    finite (usually two) number of agents. This limits the capability
    of the analysis to generalise to much larger agent populations,
    such as swarms. To allieviate this concern, the authors present an
    anaysis of the learning dynamics for a large agent population
    (which they approximate as containing infinite agents) where each
    agent is an independent Q-Learning using Boltzmann
    exploration. The emergent dynamics is given by a Fokker-Planck
    equation which is numerical shown to be a strong approximation of
    the true learning dynamics. However, as the authors point out,
    this is the first attempt at considering such a problem, and
    relies on heavy assumptions.  Possible extensions therefore
    require a consideration of:

    \begin{itemize}
        \item Asymmetric games, in which agents have different
          payoffs, introducing heterogeneity and competition into the
          system. \fr{this is not really a problem  in relation with swarms, is it?}
        \item Stateful Games. \fr{what is the interest of stateful games?}
        \item Heterogeneous populations, in which there are p different populations of agents.
    \end{itemize}

    To the best of my knowledge, in the particular case of $N >> 1$ agent populations, none of the
    above have yet been attempted.

    \section{Continuous Action Spaces}

    Here lies the opportunity to extend the work of Tuyls' et
    al. \cite{Tuyls2006AnGames} towards continuous action spaces. This
    would be particularly useful for most robotic applications, which
    operate in continuous spaces. \fr{do we have other examples?}

    Work has commenced in this area, but is still in its
    infancy. Notable examples are:
    \cite{Letcher2019DifferentiableMechanics}, in which Letcher et
    al. consider the dynamics of Differentiable Games, though their
    focus is on its application to gradient descent in GANs as opposed
    to reinforcement learning, and \cite{Galstyan2013}, in which the
    dynamics of Q-Learning, with a Boltzmann action selection is
    considered. To achieve the result, the author replaces the
    strategy vector with a probability density function (pdf) over the
    strategy space, which leads to an integro-differential equation
    which describes the evolution of the pdf with time. Due to its
    complexity, the dynamics cannot be analysed as easily as those in
    \cite{Tuyls2006AnGames}, and so Galstyan restricts the analysis to
    the steady-state solution (i.e.~the fixed point) of the
    dynamics. Their analysis shows strong agreement with the designed
    experiments, under varying payoff conditions (note that the
    payoffs are now a function, rather than a discrete matrix).

    Galstyan goes on to present avenues for further work. These are summarised below:

    \begin{itemize}
        \item  An analysis of the steady state equations, considering the existence and uniqueness
        of solutions under varying payoff structures. How do these compare with the underlying Nash
        Equilibrium. Tuyls and Westra also suggest the consideration of stability analysis in this regard.
        \item A consideration of larger agent populations. No attempt towards this has yet been made
        as far as I am aware.
        \item A consideration of these dynamics under state transitions. Tuyls and Westra suggest
        that an analysis using switching dynamics (Chapter \ref{ch::ControlTheory}) may be useful
        here, but no attempt has yet been made as far as I am aware.
    \end{itemize}

    Tuyls and Westra \cite{TuylesWestra} also consider continuous action spaces from the same
    perspective as Ruijgrok and Ruijgrok \cite{Ruijgrok}, namely that of the replicator dynamics.
    The former extend the analysis of the latter by considering the case where mutations are
    deterministic (e.g. through epsilon-greedy exploration) and these mutations only allow for small
    changes within the strategy space. From an EGT perspective, this analysis provides a strong
    characterisation of learning dynamics in continous action spaces and generalises the results of
    Galstyan to generic update rules (rather than the traditional Q-Learning approach). 

    \section{Stateful environments} \label{sec::Stateful}

    This is potentially the most relevant of the sections in regards to direct applicability to
    reinforcement learning as it extends the typical consideration of stateless normal-form games,
    in which the payoffs and strategies of each player is well defined, to stochastic games, in
    which games have probabilistic transitions across them. To the best of my knowledge, the
    existing work in this area is sparse. The earliest example is proposed by Vrancx et al 
    \cite{Vrancx2008} in which the authors extend the concept of replicator dynamics to 'piecewise
    replicator dynamics', which allows them to consider multi-state games with discrete transitions
    between them. However, this analysis places strong assumptions on the nature of the learning, in
    particular that, at each state, the strategies in all other states may be considered as fixed.
    Whilst the authors attempt to add corrections to mitigate this effect, it is clear from the
    results that the lack of state-coupling produces deficiencies in the accuracy of the model.
    Hennes et al \cite{Hennes2009} improve upon this result by incorporating state-coupling into the
    model, leading to 'state-coupled replicator dynamics'. This is proven to be a stronger indicator
    of the behaviour of their chosen learning algorithm (the Linear Reward Inaction Penalty put
    forward by Bowling and Veloso \cite{LRI}). Hennes et al. \cite{Hennes2010} go on to use the
    state-coupled replicator dynamics to reverse-engineer a learning automata algorithm, known as
    RESQ-Learning, which is shown to arrive at desired pure or Nash equilibria. 

    The state-coupled replicator dynamics provides an important step into the advancement of
    Dynamical Studies of Multi Agent Learning. However, the above appear to be the only works
    presented in this area and so its improvement is an open area of study. Potential directions for
    this include:

    \begin{itemize}
        \item The consideration of larger state spaces (the above studies only consider two).
        \item Lifting the assumption of ergodicity on the state space
        \item Extending the results to extensive form games. The closest attempt to this is
        considered by Panozzo et al. \cite{Panozzo} who consider Q-Learning in a sequence form game.
    \end{itemize}

    \fr{application of the above?}

    \section{Characterisation of Complex Dynamics}

    The intention of this area of study is to consider the ideas presented in works such as 
    \cite{Sanders2018} and \cite{Galla2011} which consider complex behaviour, including cycles and
    chaos, in certain games using simple reinforcement learning algorithms. These provide a great
    deal of insight into whether or not the game will converge and, if so, to what equilibrium.
    Galla, for instance, shows that adjusting a memory parameter when learning to solve an Iterated
    Prisoner's Dilemma game can shift the equilibrium from one showing purely defective behaviour to
    one showing cycles of cooperations and defection. Sanders et al. generalise this result by
    showing that, for a particular learning algorithm known as Experience Weighted Attraction,
    learning dynamics varies dependent on the mutual effect of two parameters. 

    This determination of where RL algorithms will show complex dynamics is important, not only for
    a priori understanding of resultant behaviour, but is particularly important for algorithms in
    which agents aim to predict the behaviour of other agents. For this to be feasible, the learning
    dynamics must not exhibit chaos, otherwise it would be impossible for an agent to make any
    reasonable predictions about the future behaviour of its opponents. 

    To this end, the potential avenue for research in this regard is to extend the ideas of Sanders
    et al. and Galla et al. towards games learnt using classical reinforcement learning algorithms 
    (such as Q-Learning with Boltzmann exploration, which seems to be a common choice). The work may
    then be able to extend beyond this, and towards continuous spaces, stateful games and large
    agent populations. This would allow for an understanding of the important parameters which
    define the behaviour of reinforcement learning algorithms and allow for a characterisation of
    the expected resultant behaviour when applying a particular algorithm to a particular game. It
    would also allow for a characterisation of the conditions under which MARL algorithms may
    feasibly be applied, thereby supporting the ability of researchers and engineers to choose their
    payoff matrices and parameters accordingly.

    \fr{this last line might possibly be the most relevant for the CDT.}

    \section{Applications}

    The ability to predict learning is crucial towards understanding the behaviour of learning
    agents. It provides a qualitative understanding of the differences between algorithms and allows
    for a better understanding of the conditions under which each algorithm is appropriate, as well
    as how to choose parameters in such a way to achieve desired effects. It can be argued
    that the above methods presume some knowledge of the payoff matrices in order to generate these
    predictions. However, a re
    
\end{document}
