% IJCAI-21 Author's response

% Template file with author's reponse

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{ijcai21-authors-response}


\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{lipsum}
\urlstyle{same}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}


\begin{document}

\paragraph{Reviewer 4} % (fold)
\label{par:reviewer_4}

The reviewer makes a pertinent point regarding the model chosen for the analysis. Indeed our
analysis is of an iterated normal-form game, which has a strong history in the learning community.
The state of the art regarding evolutionary game theoretic analysis of learning processes is not yet
at a stage where multi-stage games, or continuous state spaces can be accurately analysed and,
indeed, we remark in the conclusion that, as EGT models are improved to capture such complex games 
(such as with active environments or heterogeneous agents), we must analyse those models to
understand their implications for the stability and behaviour of learning algorithms such as
Q-Learning. This is a point for future work which we consider.

Regarding the novelty of the work, it should be noted that the references (Tuyls et al,
Bloembergen et al) focus on developing models for learning algorithms. We, on the other hand, focus
on understanding what these models tell us about the behaviour of Q-Learning. The closest analogue
to our own work is that of Leonardos and Piliouras, who show that the behaviour of Q-Learning can
vary
depending on the exploration-exploitation parameter. For this specific case, they find , as we do,
that the behaviour of Q-Learning can become rather complex. We, however, aim to analyse also the
influence that the number of players and number of actions has, which we show through both our
analysis and our experimentation.

% paragraph reviewer_4 (end)

\paragraph{Reviewer 6} % (fold)
\label{par:reviewer_6}

We thank the reviewer for their analysis and comments. Specifically, we wish to address the
reviewer's concerns regarding the averaging over payoff matrices. We should start by pointing out
that we do not average the effect over all possible games of interaction. Our aim instead is to
understand how the `type' of game (e.g. zero-sum, coordination etc) affects the stability of the
system. We make the point that, for example, there are an infinite number of zero-sum games, and so
it would not be instructive if we were to just take one zero-sum game (e.g. matching pennies) and
claim that the stability of the specific case is indicative of all zero-sum games. Rather, we choose
a parameter $\Gamma$, as was performed in (Sanders et al 2018) and (Galla and Farmer 2013), which
gives an insight into the correlations between the payoff matrices. For example, for a two-player
game, $\Gamma = -1$ corresponds to a zero-sum game, while $\Gamma = 1$ is a coordination game.

Now, with $\Gamma$ defined as such, if we are to analyse its effect on stability, we must average
over all games which share the same $\Gamma$. To only look at one would not be indicative of the
whole class. Therefore, we find the `average' dynamics for a particular choice of $\Gamma$ (i.e.
the `effective' dynamics). Similarly, in our experiment, if we are to analyse how the `type' of game
affects stability, we must average over multiple games of the same type. It is our finding that, for
Q-Learning, the type of game has almost no influence on the stability of the learning dynamics - an
interesting result which is contrary to that found for the Experience Weighted Attraction algorithm.

% paragraph reviewer_6 (end)

\paragraph{Reviewer 37} % (fold)
\label{par:reviewer_37}

The reviewer makes some very interesting points regarding the domain considered and the dynamical
aspect of our analysis. Regarding the former, it is true that we focus on the restricted domain for
$\Gamma$, as we were able to ensure that our theoretical analysis was sound for this domain.
Unfortunately, the analysis does not extend to the positive $\Gamma$ region, whose interest we
cannot neglect. Indeed you are correct in stating that the behaviour in this regime could be
analysed through a numerical study and, in fact, this is a point of work that we considered
immediately on the conclusion of our present work. However, we felt that it was important to focus
on the region in which the stability of the system could be analytically determined. We shall be
sure to clarify your point in revision.

We thank the reviewer for making their point regarding the existence of chaotic dynamics. Indeed it
is true that our results focus on the instability of the system. However, we also numerically
estimated Lypapunov exponents using the Kantz algorithm and found positive exponents for games of
large numbers of players or actions. We will be sure to make this point clear in our revision also.
Regarding the existence of periodic orbits, we conducted experiments which look for recurrent
behaviour, but were unable to find any, even for small classes of 2x2 games. A point of work which
we are looking at is to consider the possibility that, in fact, Q-Learning may not admit any
periodic solutions at all. As you mention, we will be sure to make these distinctions clear and we
thank the reviewer for pointing them out.

% paragraph reviewer_37 (end)

\paragraph{Reviewer 65} % (fold)
\label{par:reviewer_65}

We thank the reviewer for pointing out the clarification needed in the payoff matrices and that of
other RL methods. Regarding the former point, we must clarify that the payoffs themselves are not
Gaussian. Each agent receives their reward from a payoff matrix, as is typically the case in normal
form games. Our analysis requires that we understand how the structure of the payoff matrices
affects stability. As such, we parameterise the correlations between the payoff elements by
$\Gamma$. Now to truly understand the effect of this parameter, we must analyse the `average'
behaviour over all games which share the same $\Gamma$. To do this, we assert that the payoffs are
drawn from a multi-variate Gaussian, allowing us to leverage Gaussian identities in performing
averages. Therefore, we are not restricting to a particular class of payoffs, but rather
generalising over all possible payoff matrices and averaging the effect of those who share the same
payoff correlations.

Regarding the analysis of other reinforcement learning algorithms, we definitely believe that this
is an important and interesting line of inquiry. We mention in our conclusion that, as
dynamical models are developed for the algorithms you mention, analyses such as the present work
will help provide insight into what they predict regarding the stabilty and behaviour of those
algorithms.
% paragraph reviewer_65 (end)

\paragraph{Reviewer 90} % (fold)
\label{par:reviewer_90}

The reviewer makes quite useful suggestions regarding using some standard references for game
theoretic concepts. Indeed, our choice of references was focused on those which give the most
relevant insights into the results of the present work. We will, however, add these references in
our revision.

Regarding the behaviour of $\tau$, which partially encodes the propensity to exploit vs explore, we
chose not to focus on this result as it was analysed in (Leonardos and Piliouras 2020). Furthermore,
the analytic result shows that the dependence of stability on $\tau$ shares the same relationship as
$\alpha$. We shall be sure to clarify this, and we thank the reviewer for pointing it out.

Finally regarding the number of iterations for the numerical analysis and number of payoff matrices
analysed, we ensured that our analysis covered a number of initial conditions for each payoff
matrix, and that a small enough criterion for convergence was applied by visually assessing the
games to make sure they have converged. Furthermore, we can see that the numerical results
confidently confirm the predictions 1 - 5 made in the Discussion (Sec 3.4). 
  % paragraph reviewer_90 (end)
\end{document}

