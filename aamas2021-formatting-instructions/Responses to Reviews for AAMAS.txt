
Review 1: 

We thank the reviewer for their comments. 

FB: it is not entirely clear to me what are you referring to in the following paragraph. I would suggest to focus on the question for rebuttal and leave this for later. 
Regarding your point on the links to evolutionary dynamics, actually the paper by Tuyls' et al (2006) show that, by deriving the continuous time dynamics of Q-Learning, we arrive at a dynamical system which is similar to the replicator dynamic from Evolutionary Game Theory. Our goal in this paper is solely focused on understanding the stability of these dynamics. 

As regards specifically the questions for rebuttal:

1. We aim to focus our study on all normal form games. As you have pointed out, whilst the strategy space of the agents includes a finite set of actions, they are able to play mixed strategies as well as pure ones. The goal of the paper is to identify the evolution of the strategy as the agents learn. Thank you for pointing this out, we will be sure to clarify it in revision.

2 and 3. These are excellent points and we are, in fact, using the results of this study to address the problem of how to allow the agents to 'steer' the learning towards stable equilibria. However, as you have mentioned, there are cases in which there are multiple equilibria. In such cases, we have not yet determined a closed form method for steering the system towards a given one. This is an incredibly important question, however, and an important avenue for future work.


Review 2:

Thanks for the insightful remarks. Indeed, many of the techniques applied in this study are inspired by statistical physics, including the analysis of dynamical systems. We believe that such methods can be fruitfully applied to learning on games (as already shown by a wealth of works in the literature), as these contains many random variable elements: the strategies of the agents change during play, but also the payoff elements themselves can vary from game to game. We aimed, as much as possible, to focus the paper on the main, theoretical points of our contribution, while providing as many details as possible about the derivation in the supplementary material. We thank you for taking the time to look at the supplementary material, and for doing so with such rigour.

As you mentioned, the main contribution of this paper is equation (13) which establishes the region of stability. To validate this equation, we were sure to test each of its predictions, including the rather remarkable ones that independent Q-Learning rarely converges, and that the payoff correlations make no difference to stability.

As regards the style of presentation, we are aware that normally, papers at AAMAS present results in a theorem-proof format. However, we felt that this format would not be appropriate for the results described herein, as the statement of each result would just amount to the last equation in the derivation.


Review 3:

We thank the reviewer for the several points they raise, they will certainly be useful to improve the paper. They also allow us to provide some clarification, even though the reviewer does not ask any question explicitly.

The reviewer mentions that there is a disconnect between the theoretical analysis and the experimental evaluation. Nonetheless, we find that the theoretical result (13) makes a number of predictions including: the decrease of stability for increasing alpha or tau, the (rather surprising) lack of dependence on Gamma (the correlation between payoff elements), as well as the fact that the majority of the phase space is unstable. These predictions hold for the limiting case of an infinite action set and a large number of players. However, there are practical restrictions when conducting an experiment with an incredibly large action/player set. Namely, this requires storage and calculations on matrices with N^p elements. This results in severe limitations for the scale of the experiments that can be run. We do however, present a number of results in our empirical evaluation which show that the overall trend, i.e., as p and N increase. Consider, Figure 6, which considers the case of 6 players. We show that the trend is towards the theoretical predictions. It is in this sense that we mention that the size of the unstable region is overestimated by the theory, a point which was also found in [15]. The message of the paper is to highlight the sensitivity of the stability to p and N, and argue that it is required for algorithms to be analysed with stability in mind, as well as to design algorithms which focus on maintaining the stability of the system.

One of your queries regards the sentence "... in the Prisoner's Dilemma, cyclic behaviour emerges". The paper mentioned in this context is actually different from that of Figure 1. In the paper you are referencing, the authors make use of an established dynamics of the 'Experience Weighted Attraction' Learning algorithm, which shows cyclic behaviour. However, Figure 1 depicts the convergence of Q-Learning in the same PD problem. We will ensure that we clarify this distinction since it shows that different algorithms may give different results on the same problem, hence the need for such a convergence study.

Regarding the class of games considered: In this study, we aimed to make as little assumptions on the game as possible. Therefore, you are correct in saying that the payoff elements were already in a general form. However, even within this, we have the different classes of games (zero-sum, potential and everything in between). Further still, there are an infinite number of different payoff elements which could achieve a zero-sum game. We aim to parameterise the structure of the game by the Gamma, which measures the 'zero-sumness' of the game. Therefore, our study is focused on how this parameter affects the stability of the system, rather than having to vary every payoff element. It is in this sense that we make the assertion that we generalise the nature of the study to its minimal elements: the learning parameters and a parameterisation of the payoff matrix.

Your point: The convergence criterion used in the empirical evaluation should depend on the step size alpha, otherwise for a given alpha, 1% might be outside of the margin of error.

Response: (for francesco) I'm not sure what they mean by this, the empirical evaluation is already based on alpha?

Regarding the choice of tau: You are right that the values chosen are rather small. We chose these for two reasons. The first was to remain consistent with the results provided in [15], thereby justifying the comparison to it made in the discussion. Further, as our results show, the degree of instability rises as alpha and tau increase. Therefore, it is immediate to see that, for higher values of tau and alpha, the game is trivially unstable and so does not show the transition to stability.
