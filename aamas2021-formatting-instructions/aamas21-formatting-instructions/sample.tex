%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2021 (based on sample-sigconf.tex)
%%% Prepared by Natasha Alechina and Ulle Endriss (version 2020-08-06)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.
%%% Use the first variant below for the final paper.
%%% Use the second variant below for submission.

\documentclass[sigconf]{aamas} 
%\documentclass[sigconf,anonymous]{aamas} 

\usepackage{soul}
\usepackage{subcaption}


%%% Load required packages here (note that many are included already).

%%%%%%%%%%%%%%%%%%%%%%%%%%%   MACROS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\xmu}[2]{x_{#1_#2}^{#2}(t)}
\newcommand{\xmudash}[2]{x_{#1_#2}^{#2}(t')}
\newcommand{\payoff}[2]{P^{#2}_{#1_#2, #1_{-#2}}}

\newcommand{\dxmu}[1]{\dot{x}_{#1_\mu}^{\mu} (t)}
\newcommand{\hxmu}[1]{\hat{x}_{#1_\mu}^{\mu} (t)}
\newcommand{\hxnu}[1]{\hat{x}_{#1_\nu}^{\nu} (t)}
\newcommand{\hxmudash}[1]{\hat{x}_{#1_\mu}^{\mu} (t')}
\newcommand{\hxnudash}[1]{\hat{x}_{#1_\nu}^{\nu} (t')}

\newcommand{\txmu}[2]{\tilde{x}_{#1_#2}^{#2}(t)}
\newcommand{\dtxmu}[2]{\dot{\tilde{x}}_{#1_#2}^{#2}(t)}
\newcommand{\tpayoff}[2]{\tilde{P}^{#2}_{#1_#2, #1_{-#2}}}
\newcommand{\talpha}{\tilde{\alpha}}
\newcommand{\ttau}{\tilde{\tau}}
\newcommand{\htau}{\hat{\tau}}
\newcommand{\xfixed}{x_\infty}
\newcommand{\ezerof}{\eta_{0, \infty}}
\newcommand{\eonef}{\eta_{1, \infty}}

\newcommand{\xpert}{\hat{x}(t)}
\newcommand{\xpertdash}{\hat{x}(t')}
\newcommand{\ezeropert}{\hat{\eta}_0(t)}
\newcommand{\eonepert}{\hat{\eta}_1(t)}

\newcommand{\eom}{\frac{\dxmu{i}}{\xmu{i}{\mu}} - \alpha \ttau \left ( \sum_{i_{-\mu}} \payoff{i}{\mu} \prod_{\kappa \neq \mu} \xmu{i}{\kappa} \right ) + \\ \alpha \htau \left ( \frac{1}{\sqrt{N}} \sum_{i_\mu i_{-\mu}} \xmu{i}{\mu} \payoff{i}{\mu} \prod_{\kappa \neq \mu} \xmu{i}{\kappa} \right ) - \talpha \rho^{\mu}_{i}(t)}

\usepackage{xcolor} % REMOVE THIS AT THE END
\newcommand\fb[1]{\textcolor{blue}{FB: #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2021 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '21]{Proc.\@ of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021)}{May 3--7, 2021}{London, UK}{U.~Endriss, A.~Now\'{e}, F.~Dignum, A.~Lomuscio (eds.)}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{489}

%%% Use this command to specify the title of your paper.

\title[InstabilityinMARL]{Stability and Chaos in Multi Agent Reinforcement Learning}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Aamal Hussain}
\affiliation{
  \department{Department of Computing}
  \institution{Imperial College London}}
\email{aamal.hussain15@imperial.ac.uk}

\author{Francesco Belardinelli}
\affiliation{
  \department{Department of Computing}
  \institution{Imperial College London}}
\email{francesco.belardinelli@imperial.ac.uk}


%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
Modelling the dynamics of Q-Learning is an active and important topic for the sake of developing an {\em a priori} understanding of Reinforcement Learning. In this paper, we use methods from evolutionary game theory to analyse the stability of Q-Learning in $p$-player games, where payoffs are randomly generated. We determine the parameter range in which Q-Learning is expected to settle to a stable fixed point and the range in which the dynamics are unstable. 

This study allows for parameters to be appropriately chosen to ensure the safe convergence of a learning algorithm and as a first step towards understanding the range of behaviours that can be displayed by learning using the Q-Learning algorithm. We validate our theoretical results through numerical simulation and show that, within the bounds of experimental error, the region of instability can be characterised by the learning dynamics.
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Game Theory, Reinforcement Learning, Dynamical Systems}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Single agent reinforcement learning (RL) is a well established framework for allowing agents to learn optimal strategies when trained on an iterated task 
For a particularly strong example of this see \cite{Vinyals2019}. However, for the realisation of complex tasks, such as air traffic control, market negotiations and multi-robot coordination, it is required that the system be modelled as a multi-agent system (MAS). Such systems are not as well understood since a given agent is tasked with optimising a reward function which depends not only on a non-stationary environment, but also on the actions of other, possibly loosely coupled, agents \cite{SchwartzMulti-agentApproach}. 

It is, therefore, of paramount importance to develop a strong theoretical understanding of multi-agent reinforcement learning (MARL) to allow an {\em a-priori} understanding of the behaviour of a given learning algorithm. Fortunately, the study of MAS is not unique to the field of AI and has been extensively studied from the point of view of economics and game theory as well \cite{ShohamMultiagentFoundations}. In particular, the study of evolutionary game theory (EGT) considers the problem of a MAS which is repeatedly exposed to an iterated game. 
his idea shares a strong resemblance with MARL and, in fact, in \cite{Tuyls2006AnGames}, it was shown that, techniques from EGT may be fruitfully used to analyse Q-Learning \cite{Sutton2018}.

An important result in modelling such multi agent systems from the EGT perspective is that, when games are learnt and the assumptions of rationality and perfect information are lifted, games may not converge to an equilibrium. Instead, as shown in \cite{Sanders2018}, the dynamics may be more complex, and even chaotic (see Section~\ref{sec::DynamicalBehaviours}). The present work further describes that the emergence of such behaviours depends on the parameters of the games and the learning algorithm. For the analysis of multi-agent systems whose behaviour is inter-dependent, it is desirable that the system be such that the game convergence to a stable equilibrium. Without this, it is inherently impossible to predict the outcome of learning.  

\paragraph{Contribution}
In this study, we will be considering the question: under what choice of parameters is a
multi-agent system, which is trained on an iterated game using Q-Learning, likely to converge to an
equilibrium as opposed to displaying complex, or unstable behaviours? In investigating this question, we will make the
following assumptions:

\begin{enumerate}
    \item There are a finite set of agent, though the size $p$ of the set is arbitrary. 
    \item The agents have a large, but discrete, strategy space: during the theoretical study, we
    will make the assumption that the number of actions $N$ goes to infinity.
    \item The agents are homogeneous. This requires that all agents have the same parameters and are
    trained using the same algorithm. Since this is typically the case in reinforcement learning
    studies, it is not an unreasonable assumption, but does present an interesting avenue for future
    work.
    \item The agents are trained on stateless, normal form games (i.e. the environment is static).
\end{enumerate}


\subsection{Related Work}

In this section we present an overview of the vast literature which considers the study of Reinforcement Learning (RL) from an evolutionary game dynamics perspective, as well as touching upon some important results and research within the field of game dynamics.

\paragraph{Evolutionary Game Dynamics} The theory of evolutionary game dynamics \cite{Morgenstern44} considers the case in which agents must repeatedly interact with one another. The outcome of this interaction depends on a payoff matrix; 'strong' strategies which maximise the reward are promoted whilst 'weaker' strategies diminish. The \textit{replicator dynamic} models this behaviour, and allows one to determine whether, after a number of iterations, the game is likely to converge to some fixed equilibrium and, if so, the probabilities with which strategies are played in this equilibrium. \cite{ShohamMultiagentFoundations} provides an excellent introduction to the replicator dynamic for the interested reader.

Analyses have been performed from the point of view of the replicator dynamic towards the various types of behaviours that can emerge from iterated games. Such games are considered imperfect, in that agents make decisions by attempting to anticipate their opponents' behaviour based on experience \cite{Galla2011}. \cite{Imhof2005} presents the observation that, in the game of Prisoner's Dilemma, cyclic behaviour emerges in which the optimal strategy cycles across defection, cooperation and tit-for-tat despite defection being the NE. \cite{Galla2011} builds on this notion by showing that, in fact, such behaviour is highly dependent on the presence of memory loss in the system. In the absence of memory loss, the learning converges to the NE as expected. However, as this parameter is increased, the stability of the NE is removed and other stable points are created. 

Taking this notion even further,  \cite{Galla2013} describes an investigation of two-person iterated games in which the players learn using \textit{Experience Weighted Attraction} (EWA), a form of reinforcement learning whose analysis typically falls under the purview of experimental economics \cite{Camerer2009}. Through numerical simulations, the authors were able to show that, once again, the emergence of chaos and cycles (see \cite{Strogatz2000} for details on chaotic and periodic motion), is dependent on the choice of parameters. A rigorous theoretical analysis, which applies the techniques presented in \cite{Opper1992}, of the replicator dynamic corresponding to EWA presented a method for characterising the regions in parameter space in which a game is likely to converge, exhibit chaos or limit cycles. More recently, the authors presented an extended study in \cite{Sanders2018}, in which this analysis was extended towards a generic $p$-player game, which showed that chaotic dynamics are more likely to be observed as the number of players \textit{p} increases.

It is evident, therefore, that in the case of learning on iterated games, convergence to stable equilibria cannot be taken for granted and, in fact, is rare. It would therefore be fruitful to bring these analyses from EGT to better understand Reinforcement Learning from an AI perspective.

\paragraph{Dynamics of RL} In \cite{Tuyls2006AnGames}, the authors are able to derive a relation between Q-Learning (see \cite{Barber2012}) and the replicator dynamic. In doing so, the authors present an evolutionary model which accurately predicts the dynamics of Q-Learning. This sprung forward a vast array of literature which perform similar derivations for various reinforcement learning algorithms. \cite{Bloembergen2015} presents an excellent review of these. In particular it should be noted that many of these studies focus on normal form games, in which the payoff matrix does not change (i.e. the environment is static). Recent work, such as \cite{Hennes2008}, extends this work towards multi-state games, though this is restricted to 2 state games. Similarly, \cite{Galstyan2013}, extends the work in \cite{Tuyls2006AnGames} towards games with continuous action spaces. Most recently, \cite{Hu2019} considers Q-learning in the mean-field limit, in which the strategies of populations of agents are considered. 

The relation between RL and the replicator dynamic allows for the assumption of convergence in reinforcement learning to be lifted and instead for the emergence of more complex behaviour to be examined. In particular, this study aims to perform a similar analysis to that in \cite{Sanders2018} and establish the regions in parameter space in which Q-Learning converges to stable fixed points and where the learning is unstable. To the best of our knowledge, no such work exists in this context. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries}

In this section we \st{will} establish some of the preliminaries which are
required to follow the subsequent sections. The first is \textit{dynamical behaviours}, 
which considers the evolution of agent strategies as they
learn on an iterated game. This study aims to classify, based on the
agent parameters and the payoff matrices, which of these behaviours
will be observed. The second is \textit{dynamics of Q-Learning}, which are a set
of equations that model the aforementioned strategy evolution. It is
on these dynamics that we \st{will} perform our analysis.

\subsection{Dynamical Behaviours}
\label{sec::DynamicalBehaviours}

    As discussed in the previous section, when learning on iterated games, player
    strategies may exhibit much more complex behaviour than
    convergence to a Nash Equilibrium (NE),
    including convergence to a unique
    equilibrium (though not always to an NE), convergence to one of
    multiple equilibria, limit cycles and chaos. These behaviours are
    illustrated in Fig.\ref{fig::DynamicalBehaviours}. To be able to predict the behaviour of a learning algorithm, it should ideally converge to a stable equilibrium
    although it is still possible to study systems with multiple
    equilibria or limit cycles \cite{Strogatz2000}. However, it would
    be difficult to control systems whose dynamics are governed by
    chaos (though research into controlling chaos is ongoing and rife
    with opportunity \cite{Fradkov2009}). It would, therefore, be a useful endeavour to
    determine the conditions under which these sorts of behaviours
    arise.

    \begin{figure}[h]
        \centering
        
        \begin{subfigure}[b]{0.9 \textwidth}
        \includegraphics[width=0.5\textwidth]{Figures/DynamicalBehavioursTop.png}
        \end{subfigure}
        
        \begin{subfigure}[b]{0.9 \textwidth}
        \includegraphics[width=0.5\textwidth]{Figures/DynamicalBehaviourschaos.png}
        \end{subfigure}
        
        \caption{ \label{fig::DynamicalBehaviours} Different types of dynamical behaviour
       displayed
        by learning agents. Here the $x$-axis (resp.~$y$-axis) is the probability with which agent 1 (resp.~agent 2) chooses a given action. a) Figure drawn from \cite{Tuyls2006AnGames}.
        Convergence
        to a unique fixed point in the upper right corner (1, 1). This fixed point is unique, in
        that all trajectories, regardless of initialisation, will converge to this point. b) Limit Cycle, the trajectories converge to cyclic behaviour c,
        d) Chaotic behaviour, here small deviations in the initial conditions can grow
        exponentially. b-d drawn from \cite{Sanders2018}}

    \end{figure}

\subsection{Dynamics of Q-Learning}

The behaviour of a system may be studied given a model of its
dynamics. It is through this process that a wide array of physical
systems, from harmonic pendulums to geophysical fluids, can be
understood. A growing body of research aims to understand
multi-agent reinforcement learning through the lens of its
dynamics. In this light, \cite{Tuyls2006AnGames} presents of derivation of a continuous time dynamical
system describing how agents following a Q-Learning approach adjust the probabilities of choosing actions
as they iteratively play a game. The Q-Learning approach considered requires an agent to choose an action $i$ at step $k+1$ with probability

\begin{equation}
    x_i(k) = \frac{e^{\tau Q_i(k)}}{\sum_j e^{\tau Q_j(k)}}
\end{equation}

where $\tau$ is the \textit{intensity of choice} as defined at the end of this section and $Q_i$ denotes the \textit{Q-value} of an action $i$, which is to be updated at each step according to
%
\begin{equation}
\label{eqn::Qupdate}
    Q_i(k+1) = (1 - \alpha) Q_i(k) + \alpha (r + \gamma \max_j Q_j(k))
\end{equation}

where $\alpha$ is the \textit{step length} parameter defined below, $r$ is the immediate reward received and $\gamma$ is a discount factor.

Through their analysis, Tuyls et al. were able to
arrive at the following dynamical model of Multi-Agent Q-Learning
%
\begin{subequations}
\label{eqn::EOM}
    \begin{equation}
        \frac{\dot{x}_i(t)}{x_i(t)} = \alpha \tau (\sum_{j} a_{ij} y_j - \sum_{i j} x_i a_{ij} y_j)
        + \alpha \sum_j x_j ln(\frac{x_j}{x_i}) 
    \end{equation}
    \begin{equation}
        \frac{\dot{y}_i(t)}{y_i(t)} = \alpha \tau (\sum_{j} b_{ij} x_j - \sum_{i j} y_i b_{ij} x_j)
        + \alpha \sum_j y_j ln(\frac{y_j}{y_i}).
    \end{equation}
\end{subequations}

Here, $\alpha$ and $\tau$ are the parameters of the agent as described at the end of this section. Agent 1 (resp.~agent 2) takes action $i$ with probability
$x_i$ (resp.~$y_j$).
When these actions are taken, the agents
receive payoff $a_{ij}$ and $b_{ji}$ respectively. With these equations, it is possible to
predict the expected behaviour of Q-Learning agents which they go on to verify.

It is clear from (\ref{eqn::EOM}) that the long-term strategy selection of
these agents is determined by the parameters $\alpha, \tau$ and the payoffs $a_{ij}, b_{ij}$. In light of this, the following sections aim to address the question presented in the introduction: how do these elements influence the types of behaviours seen during
learning on an iterated game? Specifically, the parameters we will consider are:

\begin{enumerate}
    \item $\alpha \in [0, 1]$: the \textit{step length}. Low values of $\alpha$ denote smaller
    updates. Heuristically, we can consider this to be the memory of the agent: lower $\alpha$
    denotes longer memory.
    \item $\tau \in [0, \infty)$: the \textit{intensity of choice}, as termed by Sanders et al. 
    \cite{Sanders2018}. This is sometimes written as $\beta$ in the literature \cite{Sutton2018}.
    $\tau = 0$ results in all actions being selected with equal probability, regardless of their
    Q-value, whilst $\tau \rightarrow \infty$ results in the action with the highest Q-value chosen
    at every step. This may be considered the \textit{exploration-exploitation parameter}. For greater
    details into these parameters, the interested reader should consult \cite{Sutton2018}.
    \item $\Gamma \in [-1, p-1]$: the \textit{payoff correlation}. Since there are an infinite number of realisations of the payoffs $a_{ij}, b_{ij}$, we instead analyse the behaviour for the average payoff case. To do this, we assume that the payoff matrices are drawn from a multi-variate Gaussian with mean zero and
    covariance matrix parameterised by $\Gamma$. We then average over this Gaussian. $\Gamma = -1$ indicates a zero
    sum game, in which the sum of payoffs for a given action across all agents is 0, resulting in a
    purely competitive game. $\Gamma = p-1$ gives a purely cooperative game in which all agents
    share the same payoffs. The manner in which a game is generated from the choice of $\Gamma$ is
    described in (\ref{eqn::Payoffs}) and follows the same procedure as outlined in \cite{Sanders2018}.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stability Analysis of Q-Learning} \label{sec::Theory}

In this section we aim to determine how the choice of parameters $\alpha$ and $\tau$, alongside the choice of payoff matrix, affect the stability of the dynamics of Q-learning. However, as the values in the payoff matrix can take any real number, there are an infinite number of possible realisations of games. Of course, it would not be possible to analyse every possible game, we instead follow the procedure outlined in \cite{Coolen2005} and \cite{Galla2013} to average over these realisations and, instead, analyse the \textit{effective dynamics}, the dynamics averaged over all realisations of payoff matrices. Then, we perform a linear stability analysis around equilibrium points to determine the conditions under which a fixed point is stable.

Our first point of call is to extend the dynamics of Tuyls et al.~in  (\ref{eqn::EOM}a) and (\ref{eqn::EOM}b) to a general $p$-player game. This yields the dynamics (\ref{eqn::pEOM}). Here, player $\mu$ chooses action $i_{\mu}$ from its action space $\mathcal{A}$ at time $t$ with probability $\xmu{i}{\mu}$ and receives a reward $\payoff{i}{\mu}$ from its payoff matrix $P^\mu$ depending on its own action and the actions of all other agents $i_{-\mu}$. Formally, the notation $i_{-\mu}$ denotes the set $\{ i_\kappa : \kappa \in \{1, 2, ..., p\} \backslash \{\mu\} \}$ and $-i_{\mu}$ denotes the set $\mathcal{A} \backslash {i_\mu}$.

\begin{equation}
\begin{split}
    \label{eqn::pEOM}
    \frac{\dot{\xmu{i}{\mu}}}{\xmu{i}{\mu}} = \alpha \tau ( \sum_{i_{-\mu}} \payoff{i}{\mu} \prod_{\kappa \neq \mu} \xmu{i}{\kappa} - \\ \sum_{i_\mu i_{-\mu}} \xmu{i}{\mu} \payoff{i}{\mu} \prod_{\kappa \neq \mu} \xmu{i}{\kappa} ) + \alpha \sum_{j_\mu \in -i_\mu} \xmu{j}{\mu} ln \frac{\xmu{j}{\mu}}{\xmu{i}{\mu}}
\end{split}
\end{equation}

We now need to scale the system so that $\sum_{i_\mu} \xmu{i}{\mu} = 1$. The reason for this scaling is that, as the number of agents $p$ and number of actions $N$ increases, the expected payoffs that agent $\xmu{i}{\mu}$ receives becomes less distinguishable across its action space. Therefore, the system must be scaled to ensure appreciable differences across actions, so that the agent can choose its actions appropriately (see the discussion in \cite{Sanders2018} or the Supplementary Material for further details). Accordingly we choose scaled variables (denoted with a tilde) as
%
\begin{eqnarray*}
%    \begin{split}
        \payoff{i}{\mu} & = & \tpayoff{i}{\mu} \sqrt{N^{p-1}}\\
        \xmu{i}{\mu} & = & \frac{\txmu{i}{\mu}}{N} \\
        \talpha & = & \frac{\alpha}{N} \\
        \ttau & = & N^{-(p-1)/2} \tau \\
        \htau & = & N^{-p/2} \tau.
%    \end{split}
\end{eqnarray*}

This yields the scaled equation

\begin{equation}
\begin{split}
    \label{eqn::scaledEOM}
    \frac{\dtxmu{i}{\mu}}{\txmu{i}{\mu}} = \alpha \ttau \left ( \sum_{i_{-\mu}} \tpayoff{i}{\mu} \prod_{\kappa \neq \mu} \txmu{i}{\kappa} \right ) \\ - \alpha \htau \left ( \frac{1}{\sqrt{N}} \sum_{i_\mu i_{-\mu}} \txmu{i}{\mu} \tpayoff{i}{\mu} \prod_{\kappa \neq \mu} \txmu{i}{\kappa} \right ) \\ + \talpha \sum_{j_\mu \in -i_\mu} \txmu{j}{\mu} ln \frac{\txmu{j}{\mu}}{\txmu{i}{\mu}}
\end{split}
\end{equation}

Note that the scaled probabilities now satisfy the constraint $\sum_{i_\mu} \txmu{i}{\mu} = N$. For the remainder of this derivation we concern ourselves only with the scaled system so we drop the tilde notation on $P$ and $x$. 

As mentioned, we now wish to generalise the dynamics to account for all the possible realisations of the payoff matrix. To do this, we assert that the payoff elements (after scaling) are generated by a multivariate Gaussian distribution with mean zero and covariance given as
%
\begin{equation}
\label{eqn::Payoffs}
    \begin{split}
        \mathbb{E}\left [ \payoff{i}{\mu} \payoff{i}{\nu} \right] = \begin{cases}
        \frac{1}{N^{p-1}}  \text{ if } \nu = \mu \\
        \frac{\Gamma}{(p-1) N^{p-1}} \text{ otherwise. }
        \end{cases}
    \end{split}
\end{equation}

The motivation for choosing that the payoffs should be generated by a Gaussian is that it allows for the use of Gaussian identities when determining the average.

\subsection{Generating Functional Analysis}

We then use a generating functional approach as outlined in \cite{Mezard1986} to average the dynamics over this multivariate Gaussian. The generating functional is a method derived from statistical mechanics which allows for expectations to be taken over equations of motion and, more specifically, allows for the analysis of systems with `quenched disorder'. These are variables which are random but held fixed in time as the system evolves (in our case these variables are the payoff elements). The generating functional is given as \cite{SpinGlassTheory}

\begin{equation}
	Z = \int D[\Vec{x}] \prod_i \delta(\textit{equation of motion}_i) exp(i
	\int dt[x_i(t) \psi_i(t)]), 
\end{equation}

where the equations of motion are the Lagrange equations of motion given in (
\ref{eqn::scaledEOM}) and
the fields $\Vec{\psi_i(t)}$ and $\Vec{\phi_i(t)}$ will be set to zero at the end of the
calculation. $\delta$ denotes the Dirac delta function. Implementing our equation of motion yields

\begin{equation}
\label{eqn::separatePayoffs}
	\begin{split}
	Z(\Vec{\psi}) = \int D[\Vec{x}, \Vec{\hat{x}}] exp( i \sum_{i, \mu} \int dt [ \hxmu{i} (\frac{\dxmu{i}}{\xmu{i}{\mu}} - \talpha \rho_i^\mu (t) - h_i^\mu (t))]) \times \\ exp(-i \alpha \ttau \sum_{\mu} \sum_{i_\mu, i_{-\mu}} \int dt [\hxmu{i} \payoff{i}{\mu} \prod_{\kappa \neq \mu} \xmu{i}{\kappa} )])) 
    \times \\ exp(-i \alpha \ttau \sum_{\mu} \sum_{j_\mu, i_\mu, i_{-\mu}} \int dt [\hxmu{j}  \xmu{i}{\mu} \payoff{i}{\mu} \prod_{\kappa \neq \mu} \xmu{i}{\kappa}])) 
	\times \\ exp(i \sum_{i, \mu}
	\int dt[\xmu{i} \psi^\mu_i(t)]),
\end{split}
\end{equation}

The last two exponentials contain the payoffs of the game. These are randomly generated using a multivariate gaussian and then held fixed for the rest of the game. As such these comprise the aforementioned 'quenched disorder'. We will average over this quenched disorder (and therefore average over all possible realisations of payoff matrices). We define $Q$ to be the product of the second and third exponentials in (\ref{eqn::separatePayoffs}) 

\begin{equation}
\begin{split}
        <Q> = exp(- \frac{\alpha^2 \ttau ^2}{2} N \sum_{\mu} \Big [ \int dt dt' L^\mu(t, t') \prod_{\kappa \neq \mu} C^\kappa (t, t') + \\ \Gamma \sum_{\nu \neq \mu} K^\mu (t, t') K^\nu (t, t') \prod_{\kappa \not\in \{\mu, \nu\}} C^\kappa (t, t') \Big ] ) \times \\
        exp(- \frac{\alpha^2 \htau ^2}{2} N \sum_{\mu} \Big [ \int dt dt' L^\mu(t, t') C^\mu (t, t') \prod_{\kappa \neq \mu} C^\kappa (t, t') + \\ \Gamma \sum_{\nu \neq \mu} A^{\mu \nu} (t, t') C^\nu (t, t') C^\mu (t, t') \prod_{\kappa \not\in \{\mu, \nu\}} C^\kappa (t, t') \Big ] )
\end{split}
\end{equation}

In which

\begin{align}
\label{eqn::correlations}
    \begin{split}
        C^\mu (t, t') & :=  N^{-1} \sum_i \xmu{i}{\mu} \xmudash{i}{\mu} \\
        L^\mu (t, t') & :=  N^{-1} \sum_i \hxmu{i} \hxmudash{i} \\
        K^\mu (t, t') & :=  N^{-1} \sum_i \xmu{i}{\mu} \hxmudash{i} \\
        A^{\mu, \nu} (t, t') & :=  N^{-1} \sum_i \hxmu{i} \hxnudash{i}.
    \end{split}
\end{align}

By taking this average, described in further detail in the Supplementary Material, we obtain the \textit{effective dynamics}

\begin{equation}
    \label{eqn::EffectiveDynamics}
    \begin{split}
            \frac{1}{x} \frac{d}{dt} x(t) = \alpha^2 \ttau^2 \Gamma \int dt' \left [G(t, t')C^{p - 2}(t, t') x(t') \right ]\\ + \sqrt{2} \alpha \ttau \eta_1(t) + \sqrt{2} \alpha \htau \eta_0(t) + \talpha \rho(t), 
    \end{split}
\end{equation}

in which we have assumed that all players' actions are independent and drawn from the same initial distribution (i.i.d)
and therefore dropped the distinction between players and strategy components. The terms $G, C, \eta_1, \eta_0$ are correlation functions, generated during when averaging the Gaussian. These are given by 

\begin{eqnarray*}
%    \begin{split}
        C(t, t') & = & \mathbb{E}[x(t) x(t')] \\
        \mathbb{E}[\eta_1(t)] & = & 1, \text{ \space } \mathbb{E}[\eta_1(t) \eta_1(t')]  =  C^{p-1}(t, t') \\
        \mathbb{E}[\eta_0(t)] & = & 1, \text{ \space } \mathbb{E}[\eta_0(t) \eta_0(t')] = C^{p}(t, t') \\
        G(t, t') & = & \mathbb{E}\left [ \frac{\delta x(t)}{ \delta \eta_1(t')} \right].
%    \end{split}
\end{eqnarray*}

It is important to note the effect of the former assumption (that all actions of all agents are i.i.d) is substantial. With this assumption, we identify the coupled term $A^{\mu, \nu} (t, t')$ in (\ref{eqn::correlations}) with the uncoupled term $L^\mu (t, t')$ (since there is no distinction between the actions of agents $\mu$ and $\nu$). This is a strong assumption which removes the interdependency between agents in the analysis. However, as shown by the experimental evaluation, it does not produce a strong discrepancy in describing the qualitative effect on stability caused by the parameters $\alpha, \Gamma, \tau$.


\subsection{Linear Stability Analysis}

We now analyse the stability of this system in a neighbourhood around the fixed point. This follows a similar procedure as in \cite{Opper1992}, in which the fixed point dynamics are proposed to be perturbed by a disturbance $\xi(t)$ which is drawn from a Gaussian of zero mean and unit variance. The disturbance causes the values of $x(t)$ and $\eta_0(t), \eta_1(t)$ to deviate from their fixed point position $\xfixed, \ezerof, \eonef$ by an amount $\xpert, \ezeropert, \eonepert$.
If we consider only the terms which are linear in these perturbations, we arrive at the dynamics

\begin{equation}
\label{eqn::Linearised}
    \begin{split}
        \frac{d}{dt} \xpert = (\xfixed + \xpert) [ \alpha^2 \ttau^2 \Gamma \xfixed \int dt' [ G(t, t')C^{p - 2}(t, t') ]\\ + \sqrt{2} \alpha \ttau \eonef + \sqrt{2} \alpha \htau \ezerof + \talpha \rho] \\
        + \xfixed [\alpha^2 \ttau^2 \Gamma \int dt' [ G(t, t')C^{p - 2}(t, t') \xpertdash ] \\ + \sqrt{2} \alpha \ttau \eonepert + \sqrt{2} \alpha \htau \ezeropert + \xi(t)].
    \end{split}
\end{equation}

At a fixed point, then, the perturbations $\xpert$ will be zero, leaving

\begin{equation}
    \begin{split}
    \label{eqn::fixed_point}
        0 = \xfixed [ \alpha^2 \ttau^2 \Gamma \xfixed q^{n-1} \chi + \\ + \sqrt{2} \alpha \ttau q^{n/2}z + \sqrt{2} \alpha \htau q^{(n+1)/2} z^{(n+1)/n}],
    \end{split}
\end{equation}

where $\chi = \int dt' G(t - t')$, $\eta_0(t) = q^{n/2}z$ and $z$ is drawn from a Gaussian of zero mean and unit variance and $n = p - 1$. This gives us an expression for calculating $\xfixed$. We disregard the choice $\xfixed  = 0$, since we assume that no action will be chosen with exactly zero probability, an intuitive assumption which is also considered in \cite{Sanders2018} and \cite{Coolen2005}. The expression inside the squared bracket admits a positive solution only in the region $\Gamma \in [-1, 0]$, whilst for positive $\Gamma$, the nature of solutions may not be guaranteed. Therefore, we restrict our analysis to this region. 


Having arrived at a condition for the fixed point, we then examine the long-term behaviour of the perturbation $\xpert$. To do this, we take the Fourier transform of (\ref{eqn::Linearised}) and analyse its behaviour at $\omega = 0$. After some manipulation, which are given in the Supplementary Material, this gives

\begin{equation}
\begin{split}
        \mathbb{E}[|\mathcal{X}(\omega = 0)|^2] = \frac{1}{\left[ (-\alpha^2 \ttau^2 \Gamma q^{n-1} \chi)^2 - 2 (\alpha \ttau + \alpha \htau)^2 (p-1)q^{p-2}\right]} \\
\end{split}
\end{equation}

As the left hand side of this equation must be greater than zero, we arrive at the condition that, at a fixed point, the long term perturbations must satisfy

\begin{equation}
    \label{eqn::Final}
    0 \leq \left [(\alpha^2 \ttau^2 \Gamma q^{n-1} \chi)^{2} - 2 (\alpha \ttau + \alpha \htau)^2 (p-1)q^{p-2} \right ]
\end{equation}

It should be noted that, to arrive at this result, we have had to take the assumption that $2 >> 1/n$. 


\paragraph{Discussion}
The fixed point condition (\ref{eqn::Final}) yields a number of testable implications, of which we test the validity through in the subsequent section. These implications are as follows.

\begin{enumerate}
    \item A trivial result is that the game is everywhere convergent (i.e. regardless of the choice of $\Gamma, N, p$) for the cases of $\alpha$ = 0 and/or $\tau$ = 0.  We see that choosing these values would result in the right hand side of (\ref{eqn::Final}) yielding a value of zero which falls within the stable region.
    \item Convergence is rare. This is seen by the fact that the analytic result overestimates the region of instability. In fact, for all allowed choices of $\Gamma, \alpha, \tau$, the right hand side of (\ref{eqn::Final}) is negative, which violates the stability criterion. This implies that games, in general, will not converge.
    \item The likelihood of convergence decreases with increasing $\alpha$ and $\tau$, regardless of the choice of $N, p$. We can see this since the right hand side of (\ref{eqn::Final}) tends further away from zero (i.e. further from the region of stability).
    \item The choice of $\Gamma$ does not affect the stability of the game in the negatively-correlated regime. Therefore, the only dependent factors are $\alpha, \tau, N, p$. This is more easily interpreted from the heatmaps in Figure \ref{fig:theory}.
    \item The likelihood of convergence decreases as $p$ increases. We see this by noticing the weighting that the $(p-1)$ in the second term of (\ref{eqn::Final}) places. As $p$ increases, this term drives the system further from the stability boundary.

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Evaluation} \label{sec:exev}

\begin{figure}
    \centering
    \includegraphics[width = 1.1 \linewidth]{Figures/Theory.png}
    \caption{Results of numerical experiments. The heatmaps show the value of the Liapunov Exponent determined by the procedure outlined in Section~\ref{sec:exev} scaled by $1 \times 10^5$ for convenience. Lighter values show a higher exponent whilst darker values indicate lower values. Each experiment is run with $\tau = 0.05$. (Top Left) $p = 2, N = 2$, (Top Right) $p = 2, N = 4$, (Mid-Left) $p = 2, N = 10$, (Mid-Right) $p = 2, N = 20$, (Bottom) $p = 3, N = 5$.}
    \label{fig:theory}
\end{figure}
%\fb{Can we arrange Fig.~\ref{fig:theory} in a 2x2 square?}

We illustrate these implications in Figure \ref{fig:theory} which plots the value obtained by the right hand side of (\ref{eqn::Final}). In order to determine these values, we first had to solve (\ref{eqn::fixed_point}) for $q$ and $\chi$. We determine the $\xfixed$ which solves the expression inside the square brackets of (\ref{eqn::fixed_point}) using a Newton-Raphson root-finding approach and use the following expressions for $q, \chi$ to determine their value. 

\begin{equation*}
    \begin{split}
        1 = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} \xfixed(z) exp(-\frac{z^2}{2}) dz \\ 
        q = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} \xfixed(z)^2 exp(-\frac{z^2}{2}) dz \\ 
        \chi = \frac{1}{q^{n/2}}\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} \frac{\partial \xfixed (z)}{\partial z} exp(-\frac{z^2}{2}) dz
    \end{split}
\end{equation*}

It should note that this method of finding $\xfixed$ yields only an approximate value and so the $q, \chi$ which are found are estimates of their true value. 

We consider each of the implications 2-5 from the previous section in turn, choosing to forego illustrating 1 as it can be seen trivially from the expression. In those heatmaps for which $\tau$ is held fixed \textbf{denote them}, we choose $\tau = 0.05 $ and vary $\Gamma$ in the range $[-1, 0]$ and $\alpha$ in the range $[0.01, 0.05]$. For those in which $\alpha$ is held fixed, \textbf{denote them} we choose $\alpha =  0.05$ and vary $\tau$ in the range $[0.01, 0.05]$.

By looking at the values of the heatmap, we see that, for all choices of the parameters, other than $\alpha = 0, \tau = 0$, (\ref{eqn::Final}) evaluates to a negative value, which signals instability. As such, the analysis over-estimates the region of instability. We believe that this is due to the heavy assumptions made during the derivation of the analytic result, the strongest of which was to drop the discrepancy between players and actions, treating each action of each player to be independent and identically distributed. However, even within the error that this assumption generates, it is still clear (and is verified experimentally) that convergence in Q-Learning is rare. Indeed as the choice of $N$ increases, the stability boundary determined by experiment (Figure \ref{fig:NumericalExperiments}) tends towards $\alpha = 0, \tau = 0$. 

Similarly, as the value of $N$ increases, the values in the heatmap (seen on the right of the map) reduce. This is due to the assumption made in the derivation of the analytic result that $N \rightarrow \infty$. As the value of N increases, the discrepancy between the true behaviour of the system and analytic result decreases. This does, however, give rise to a limitation of the result - namely that the effect of the number of actions $N$ cannot be accurately inferred from the (\ref{eqn::Final}). However, it should still be noted that, even with the increase of N, the values in parameter space remain negative and still, accurately, predict instability. We go on to verify that the effect of $p, \alpha, \Gamma$ and $\tau$ are accurately captured by (\ref{eqn::Final}).

We interpret (\ref{eqn::Final}), for given parameters, by determining how close the right hand side is to the stability boundary (i.e., how close the value is to zero). A choice of parameters which evaluates close to the stability boundary has a higher chance (by the analytic error) of being convergent than one which lies further from the boundary. Therefore, the heat-maps in Figure \ref{fig:theory} should be considered to show the probability of convergence for a choice of $(\alpha, \Gamma)$. Lighter regions denote a higher probability of convergence whereas darker regions are those whose values are further from zero and therefore show a lower probability of convergence.

We can visualise the implication 3 by noticing that the likelihood of convergence decreases (the heatmap becomes darker) as $\alpha$ increases, and similarly for $\tau$. This occurs independent of the choice of $N$ or $p$. The implication 4 can be inferred by the trend of convergence, which occurs only in varying $\alpha$ and $\tau$. The choice of $\Gamma$
makes almost no difference in the evaluation of (\ref{eqn::Final}). We anticipate that this is due to the restriction of $\Gamma$ to be in the range $[-1, 0]$. In this range, the scaling effect of $\Gamma^2$ is dominated by the $\alpha^4$ and $\tau^4$ terms which appear in (\ref{eqn::Final}).

\begin{figure}[t]
    \centering
    \includegraphics[width = 0.9 \linewidth]{Figures/p variation.png}
    \caption{Evaluation of (\ref{eqn::Final}) for varying numbers of players $p$. Here, $\alpha$ is fixed at 0.02, $\tau = 0.05$, $N = 2$ and $\Gamma = -1$. $p$ is varied from 2 to 12. We see that the value obtained by (\ref{eqn::Final}) tends away from zero monotonically.}
    \label{fig:pvariation}
\end{figure}

Finally we can see implication 5, by considering Figure \ref{fig:pvariation}. In this we keep all parameters, except for $p$, fixed and evaluate the right hand side of (\ref{eqn::Final}) for varying $p$. We see that the trend is to tend away from the stability boundary, indicating that as the number of players in the system increases, so too does the region of instability.

\subsection{Construction of Numerical Experiments}

To verify experimentally the theoretical results, and to examine the underlying structure of stability and chaos in
Multi-Agent Q-Learning, we perform a series of numerical experiments by varying the parameters $\Gamma$ and
$\alpha$ whilst keeping $\tau$ fixed. The aim is to determine the regions in which games learnt using Q-Learning converge to an equilibrium. The results of these experiments are shown in Figure \ref{fig:NumericalExperiments}, with parameters chosen to match those in the analytic assessment (Figure \ref{fig:theory}).
% By examination it is found that, in many cases the system oscillates about some equilibrium point in a low dimensional chaotic manner (i.e. the manner in which the action probabilities change is indistinguishable from random motion) \textbf{get some graphs for this}



% The purpose 
% \st{of this experiment} is to determine a heuristic estimate for the Lyapunov exponent, which measures the degree to which initial conditions which start close to one another converge or diverge as the dynamics evolve \cite{Strogatz2000}. The former indicates a stable system (in the sense of Lyapunov \cite{}) whilst the latter signals instability. The procedure for calculating the Lyapunov exponent is given in (\ref{eq::LiapExpo}).

To generate the numerical simulations in Figure~\ref{fig:NumericalExperiments} we used the
following procedure. Note in the following that for any heatmap in which $\tau$ is swept over, the roles of $\alpha$ and $\tau$ are swapped.
\begin{enumerate}
   \item Fix the parameters $\tau, \gamma$. The former is held at 0.05 and the latter at 0.1. For the heatmaps in which $\tau$ is swept over $\alpha$ is held fixed at 0.05.
   \item Initialise values of $\Gamma, \alpha$ (or $\tau$ appropriately). These will be swept over in the experiment.
\item Generate 15 payoff matrices for both agents by sampling from a multi-variate Gaussian 
(variables are the payoff elements) with mean zero and covariance parameterised by $\Gamma$.
\item For each of these payoff matrices, initialise a set of agents with random initial conditions (i.e., random action probabilities).
\item Allow both sets of agents to learn over a maximum of $1.5 \times 10^4$ iterations.
\item Keep track of the action probabilities over a window of 500 iterations. At the end of each window, determine the percentage difference between the maximum and minimum values of each strategy component
\item If the difference is less than 1 \% consider the game converged. Otherwise continue to the next window.
\item If the game reaches $1.5 \times 10^4$ iterations without satisfying the relative distance criterion, consider it to be non-convergent. Determine the fraction of these 15 games which have converged.


% \item After 500 iterations measure the distance $\Vec{\delta_1} = |A_2 - A_1|$
% \item After 10000 iterations measure the distance $\Vec{\delta_n} = |A_2 - A_1|$
% \item Determine \fb{the Lyapunov exponent} 
% \begin{eqnarray*}
% \label{eq::LiapExpo}
%     \lambda_n & = & \max_{i} 10^{-4} ln(\delta_{n, i}/\delta_{1, i})
% \end{eqnarray*}
% for each action $i$ and average this value over all five realisations of the payoff matrix. 
\end{enumerate}
   
We see in Figure \ref{fig:NumericalExperiments} that the stability of the system is highly dependent on
the value of $\alpha$ and $\tau$ but not on $\Gamma$.
   
\begin{figure}[t]
    \centering
    \includegraphics[width = 1.2 \linewidth]{Figures/Experiments.png}
    \caption{Results of numerical experiments in which $\alpha$ is varied. The heatmaps show the fraction of games which converge to an equilibrium. Lighter values show higher convergence. Each experiment is run with $\tau = 0.05$. (Top Left) $p = 2, N = 2$, (Top Right) $p = 2, N = 5$, (Bottom Left) $p = 2, N = 20$, (Bottom Right) $p=3, N = 2$.}
    \label{fig:NumericalExperiments}
\end{figure}


\subsection{Discussion}

We note that the numerical experiments confirm the predictions suggested by the analytic results shown in Figure \ref{sec::Theory}. We notice that convergence occurs almost only for low values of $\alpha$. This observation remains as we increase the value of $N$. As noted, the analytic result over-estimates the region of instability due to the assumptions of $N \rightarrow \infty$ and $\frac{1}{p-1} << 2$ made during the derivation. Remaining discrepancies are partially due to the aforementioned approximation in the calculation of $q$, $\chi$ and the fact that the analytic result considers a continuous time approximation of the expected behaviour of Q-Learning. We expect that testing over a greater number of payoff realisations and initial conditions will yield a more representative assessment of the average behaviour of Q-Learning. However, running these experiments is a computationally expensive procedure: for $p$ players and $N$ actions we require operations on $p$ matrices with $N^{p}$ elements. As such, due to a reduced availability of computational facilities, a large scale averaging was not possible.

A point which we wish to discuss here is the difference in the result found w.r.t.~\cite{Sanders2018}, which considered the stability of \textit{Experience Weighted Attraction} (EWA). Namely, Sanders et al.~found that convergence is seen for higher values of $\alpha$, whereas lower values give rise to chaos, the opposite of what is found here. The reason for this can be seen in the update equation of Q-Learning (\ref{eqn::Qupdate}), whereby
smaller values of $\alpha$ result in the agent placing a lower weight on the reward received at each step. As such, lower values of $\alpha$ result in the agent taking more conservative steps and yields a higher probability of convergence. In contrast, the update for EWA does not discount the reward received at all and, instead, only discounts the previous knowledge of the Q-value based on higher choices of $\alpha$. In essence, the difference is the fact that Q-Learning discounts new information by a factor of $\alpha$, whilst EWA does not. Yet, the difference in stability caused by this change is significant. This highlights the importance of performing analyses such as the present work; it allows for a method to analytically compare the difference between learning algorithms and, for practitioners, ensure that the appropriate algorithm is chosen for the parameters of their specific task.

To summarise, we have shown that the analytic result (\ref{eqn::Final}) provides a strong assessment for the effect that $\alpha$, $\tau$, $\Gamma$ and $p$ have on the likelihood of convergence of Q-Learning. We show that, at least for negatively correlated payoffs, the strength of correlation plays no role in determining the stability of the system. Furthermore, though (\ref{eqn::Final}) overestimates the region of instability, it accurately conveys that convergence of Q-Learning is unlikely for games with many players and actions. In fact, our experimental results also verify that stability may only be guaranteed for the limiting case of two-player, two-action games.

Finally, we show that the behaviour of the stability line differs from that of EWA and suggest that this shows that a stability analysis is an important mode of analysis for reinforcement learning equations to ensure that parameters are being chosen appropriately to guarantee the safe convergence of learning. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

In this study, we characterise the behaviours of Q-Learning in $p$-player, $N$-action games. To do this, we analyse the replicator model of Q-Learning derived by Tuyls et al. Specifically, we search for the regions in parameter space where the dynamics are expected to converge to a stable equilibrium and those where learning is unstable. This yields a number of important results. We find that convergence to a unique fixed point is found for low values of $\alpha$, the \textit{step length} of the algorithm and for negatively correlated payoff matrices, though the strength of correlation $\Gamma$ does not influence stability. As $\alpha$ increases, the likelihood of convergence decreases. The same is true for the \textit{intensity of choice} parameter $\tau$. Our analysis also shows that the likelihood of convergence decreases, regardless of parameter choice as the number of players $p$ in the system increases.

As future work, our first aim is to verify the analytic result at a finer resolution and by averaging over a greater number of payoff realisations. This will give a greater insight into the boundary between convergent and non-convergent behaviour and allow for practitioners to choose their parameters in such a way that convergence to a unique fixed point is expected. In addition we noted that one of the conclusions to our study is that convergence to a unique fixed point is rare in Q-Learning. In fact, complex behaviour (such as the existence of multiple fixed points, limit cycles and chaos) is more likely. We, therefore, aim to characterise these complex behaviours in parameter space both theoretically and numerically.

The present work gives a first step towards considering the stability of Multi-Agent Reinforcement Learning and focuses on the limiting case of stateless normal-form games played by homogeneous agents. As research into the dynamics of Reinforcement Learning algorithms progresses, it would be prudent to apply this analysis to various other algorithm. Algorithms whose dynamics are established, and are therefore open to a stability analysis, include piecewise Q-Learning and Cross Learning. This would provide a strong method by which to compare and provide safety guarantees to different algorithms for a particular use case. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The acknowledgments section is defined using the "acks" environment
%%% (rather than an unnumbered section). The use of this environment 
%%% ensures the proper identification of the section in the article 
%%% metadata as well as the consistent spelling of the heading.

\begin{acks}
If you wish to include any acknowledgments in your paper (e.g., to 
people or funding agencies), please do so using the `\texttt{acks}' 
environment. Note that the text of your acknowledgments will be omitted
if you compile your document with the `\texttt{anonymous}' option.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


