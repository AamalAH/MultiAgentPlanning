\documentclass[preprint,8pt]{report}

\usepackage{matlab-prettifier}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[margin=0.5in,letterpaper]{geometry}
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 
\usepackage{listings}
\def\changemargin#1{\list{}{\leftmargin#1}\item[]}
\let\endchangemargin=\endlist
\title{Literature Review - Informal}

\author{Aamal Hussain}

\date{\today}

\begin{document}

\maketitle
\tableofcontents

\chapter*{Introduction}

\textbf{Note: This review is a work in progress. Please expect an update on the Game Theoretic, Hard Coded and Multi Agent Dynamics sections of this review within the week. Verification in swarms may take a little more time for me to catch up with.} \newline

The areas of interest regarding Multi Agent Systems fall into two distinct categories (which have some slight overlap). The first is denoted as 'Decision Making', though the terms 'Planning' and 'Distributed Control' may sometimes be used as substitutes. It is important to note that, in my case, I do not include Planning Formalisation techniques such as PDDL. Instead, I focus on the particular methods which involve interaction in the world. The second, I will refer to as 'Multi Agent Dynamics', though sometimes I will refer to this as 'Stability Analysis'. 

\subsection*{Scope}

Both of the aforementioned topics have been applied to all variants of multi agent systems and have sufficient room for further exploration. The typical variants of multi agent systems that I will consider are

\begin{itemize}
    \item Swarms
    \item Multi Agent Reinforcement Learning (MARL)
    \item Decentralised Partially Observed Markov Decision Processes (Dec-POMDP)
    \item Game Theoretic Approaches
    \item Hard Coded
\end{itemize}

In the above, 'Game Theoretic Approaches' cover a wide spectrum, including zero-sum (minimax) games, bayesian games, and many more. The final category, 'Hard Coded', refers to any approach which does not fit neatly into any of these categories. The name is chosen since they often refer to methods which revolve around a series of if-else statements.

I will first consider Decision Making as this is most closely related to the research proposal, before I consider Multi Agent Dynamics, which has strong implications for Safe and Trusted Multi Agent Systems. 

\subsection*{Objectives}

The aim of the following sections is not to provide an exhaustive list of all work done in the aforementioned areas. To attempt to do this would be an exercise in futility. Instead, it is to identify research directions which lie within the broad scope of Multi-Agent Systems (MAS). With these directions, we may be able to narrow down the remaining literature review and hone in on particular problems and, in fact, we may find that we address multiple of these over the next four years (while we, of course add more). As such, the end of each chapter will provide a list of research directions which I have identified from the preceding review. 

\chapter{Decision Making}

Decision making refers to the generalised problem of considering how a multi-agent system should interact in the world to achieve their goals. This subsumes both the cases where the agents' goals are aligned (cooperative) or in conflict with one another (competitive). 

The following sections will provide an overview of the literature aimed towards coordinating such systems as followed by interesting sub-fields in each approach which present avenues for research related to Safe and Trusted AI (STAI). These, of course, are not exhaustive and more will be added as the review progresses.

\section{Swarms}

Swarm based systems comprise of multiple homogeneous agents which are able to organise themselves through a formation using a series of simple local interactions with their neighbours \cite{Couceiro2015}. Whilst the individual agents are generally simplistic, the collective behaviour may exhibit complex phenomena emulating systems observed in biological organisations such as bee or ant colonies \cite{Sethi2017}. Hybrid algorithms such as in \cite{Gao2018} show an accelerated performance in reaching globally optimal solutions in search-based tasks. The advantage of many swarm algorithms is that they are based on local interactions and so are incredibly scalable \cite{Rizk2018}. 

The reduction of the complexity in interactions between agents also allows for the robots to perform other calculations on board. In \cite{Pini2011TaskSelection}, Pini et al. leverage this by considering adaptive task partitioning across swarms. This allows a swarm, in a decentralised manner, to deliberate whether to partition a task into its sub-tasks or to perform the task in its whole. As of now (to the best of my knowledge) the problem of partitioning general tasks into its N sub-tasks is unexplored. This, however, highlights another advantage of swarm systems; they are readily divided into sub-groups (as in \cite{Zahadat2016DivisionInhibition}) to perform a divide-et-impera (divide and conquer) approach to solving problems \cite{Pini2011TaskSelection}). 

Furthermore, swarm systems may be designed in a leaderless manner and so do not require the use of a central controller \cite{Couceiro2015}. This presents the advantage that the system can rapidly adapt to the loss of agents or separation of groups throughout the task. However, the assumptions made regarding the homogeneity of individual agents and the simplicity of their local interactions result in significant limitations placed on the complexity of the tasks that swarm systems can accomplish. 

\subsection{Co-evolution and Self-Healing}

Recently, there has been an increased interest in introducing heterogeneity into the swarm systems to improve their real world applicability. An example of this which have been shown strong real world success can be found in \cite{DucatelleSelf-organizedSwarms}. Here, the authors consider two swarm teams, referred to as 'foot bots' and 'eye bots' who work in unison to explore an environment and solve a navigation task. 

This area of research is sparsely populated and warrants further exploration. This is since the use of co-evolutionary teams can improve the robustness of the swarm optimisation. This is since it will be possible for a team to automatically determine when robots in the other team are not exhibiting expected behaviour and ensure that the other team self corrects. This process is referred to as 'Self-Healing' in \cite{LiuTrust-Aware}. Here, the authors allow a user to define the goals of a swarm system. From this, a 'trust' metric can be defined which measures the deviation of each agent from the expected behaviour. The self-healing process occurs by limiting communication of all agents with 'untrusted' agents and encouraging communication with 'trusted' agents. The unison of self-healing with co-evolution may present the opportunity for heterogenous swarms to maintain their evolutionary stability, even in the face of environmental disturbances.

\subsection{Fault Detection}

The above sections have considered the fact that swarm systems are robust to losses in the group. However, any MAS system must first be able to recognise that an agent has undergone some failure. 

To this end, Tarapore et al. \cite{Tarapore2019FaultDetection} develop a robust fault detection approach in which the swarm itself, in a decentralised manner, is capable of assessing deviation from 'normal' behaviour, even when the behaviour of the swarm itself is altered (perhaps by a remote operator). The authors achieve this by requiring that the agents themselves sense and characterise their own behaviour. This characterisation is formulated as a binary feature vector which is then communicated to the agent's neighbours. These neighbours will reach a consensus over whether the agent should be treated as faulty based on their collective behaviour. The results presented in \cite{Tarapore2019FaultDetection} show extremely promising results and suggest that their method is, in fact, able to determine faults with high accuracy in the presence of various fault types (including sensor and actuator failures), although poor performance is seen in actuation failures in some instances. It should be noted that this method requires that each robot transmit their feature vector to the nearest neighbours. In environments where communication may be severely limited, this may present further errors. Furthermore, it is unlikely that, when a robot is damaged, only one of its components will be affected. Therefore, it is important to determine the effect on performance in the face of multiple agent failures and in communication losses. This exploration may open the possibility of improving the state of the art in terms of failure detection in swarm systems. (Of course, this conclusion is based off two papers so further review is required).

\subsection{Verification}

Both of Alessio's papers \cite{Kouvaros2019FormalSystems, Lomuscio2019ASystems} fit in here but they require further reading

\subsection{Directions}

From the above consideration of swarms, the following research directions have been identified which, in my view, concern themselves with STAI.

\begin{itemize}
    \item Healing through co-evolution: The application of heterogenous robot swarms towards ensuring that emergent phenomenon and swarm behaviour are as expected by the user.
    \item Fault Detection in limited communication: Considering the ability of a swarm to, in a decentralised approach, consider which robots in the team have failed, even in cases of no communication or multiple failures.
\end{itemize}

\section{Dec-POMDPs}

The use of POMDPs in multi-agent settings is formalised as decentralised POMDP (Dec-POMDP) which aims for a team of agents to maximise a common utility. However, it has been found that determining the exact solution to Dec-POMDP problems is NEXP \cite{Eker2011} and so is intractable for all but toy problems. A number of methods have been presented to attempt to solve Dec-POMDPs. Oliehoek gives a review of these in \cite{OliehoekDecentralizedPOMDPs}. Most solutions (such as brute force) are intractable for all but toy problems. 

Approximate solutions to Dec-POMDP have been proposed, perhaps most notable of which is the proposal of MacDec-POMDP \cite{Amato2015} by Amato et al. Here, macro actions (actions which extend over multiple time steps) are used, as opposed to low-level actions which are re-evaluated at each time step. This allows an exact solution to be found as it does not need to be evaluated at each time step. This method assumes that, once macro-actions are distributed, the policies (sequence of state-action pairs) are known. Since this is not the case, Amato also proposes the use of a Dec-POSMDP \cite{Amato2017Decision-MakingLearning}, where 'SMDP' refers to 'Semi-Markov Decision Process, in which a high level model is defined without the underlying Dec-POMDP's actions and observations.

However, this is largely applicable in passive settings where common payoffs can be determined by an offline planner. They also require a significant amount of data with which to allow the system to learn the underlying models and payoff structures. This limits the applicability of the system when communication is limited and the system is presented with environments that it has not seen before. Recent work in MDPs \cite{Klima2019RobustDomains} has considered learning in the face of Significant Rare Events (SREs) which the system has not yet observed. Currently, it is required that a model of such SREs is known and so it would be interesting to consider the application of Dec-POMDPs in situations where the SRE model is incomplete or erroneous and assess the robustness of the Dec-POMDP framework against such events. 

\subsection{Directions}

The area of Dec-POMDPs still requires more review from me, especially for solutions which are not developed by Amato as he seems to largely dominate the field. Based on this initial review of the area, a potential direction for research is

\begin{itemize}
    \item Significant Rare Events: Consider Dec-POMDP capability to remain robust to SREs which have a limited or erroneous model.
    \item Agent Failures: The Dec-POMDP model uses a centralised planner which acts offline. It therefore assumes that the agents will be able to carry out their assigned tasks. It would be interesting to examine the possibility of, either adaptive planning, or planning with contingencies in the Dec-POMDP framework.
\end{itemize}

\section{Game Theoretic Approaches}

Game theoretic models are generally the go-to method for understanding multi-agent systems. As such they fit into all of the categories in this chapter (except, perhaps, swarms) since Dec-POMDP and MARL methods have both used game theory to support their frameworks. In fact, Dec-POMDP is a subset of Partially Observed Stochastic Games (POSG), in which all agents use the same payoff. Game theory can, therefore, be used in an applied capacity to direct task allocation across heterogenous teams.

\subsection{Market Based Methods}

Garapati et al. \cite{Garapati2018AMissions} define a market based method as the setting where agents "follow their own interests and establish the mechanism of a market for distributing the tasks". Auctioning is the most widely used sub-field of market approaches and so I will use them interchangeably. 

Whilst there are different variants to autioning, the general procedure is that an auctioneer who has knowledge of a task (or multiple tasks) will set up an auction for said task. Agents can then make bids on these tasks and, once the auction is complete, the highest bid will win the task. In the specific application to robotics, a robot's bid will often reflect the costs, suitability or utility their undertaking the task \cite{BernardineDias2006Market-basedAnalysis}. This immediately highlights a few points. The first is that the method is not too heavily reliant upon a single processor to determine some joint policy. Tasks are allocated on a case-by-case basis and the utilities are calculated by the agent themselves. The only centralised process is the auctioneer's assessment of the winner which then relays this information back to them. The downside of this is that the system is heavily reliant upon strong communication channels, without which tasks may not be assigned, incorrect utilities may be communicated and, in general, sub-optimal solutions reached. Furthermore, the requirement that the agents themselves determine the cost of their actions assumes that they have the computational capability to do so. Furthermore, the bids placed by each agent need to be a strong representation of their capability to perform a task which may be hard to estimate without expert knowledge. However, market based methods are well suited to explanation through argumentation (similar to \cite{Jung2001DistributedArgumentation}. 

With well chosen payoffs, market based approaches work extremely well. For instance, in \cite{Dias2000ASystem}, the authors show that a free market approach (where agents try to maximise their own profits) can lead to a strong collaborative effort across teams. Similarly, in \cite{Thomas2005Multi-robotScenarios}, Thomas et al. apply the auctioning scheme presented in \cite{Gerkey2002Sold:Coordination} towards a robot construction team. However, it is important to note that these are both passive settings; tasks were assigned before the team were in the field and, in the case of \cite{Gerkey2002Sold:Coordination}, the system would repeat the bidding process if a robot failed. While both show strong performance, it cannot be said that either would be applicable in dangerous environments in which dynamic reassignment must happen within strict time constraints. Stancliff et al. \cite{Stancliff2009PlanningAllocation} suggest that a more robust method to planning would be to account for failures a priori, a philosophy which is exemplified in \cite{Chen2010ACollaboration} who consider the robot's reliability and relevance to a task as well as 'history relevance' which considers the relationship between pairs of robots with the aim of producing more effective teams.

There has also been some interesting work in probabilistic verification of market based approaches. Most notable to me is \cite{Pallottino2007ProbabilisticAvoidance} which considers the case of conflict avoidance. Though their method focuses on collision avoidance, it highlights the need for verification of conflict resolution and goal achievement in market based approaches with different payoff structures. Sirigineedi et al. \cite{Sirigineedi2010DecentralisedApproach} make a step in this direction by considering the verification of cooperative surveillance along a route network. From my understanding, this means that they were able to verify that their agents were able to traverse along the network without interference. However, this, as always, requires further analysis to truly understand.

\subsection{Directions}

The particular considerations which have jumped out to me from the above analysis are as follows:

\begin{itemize}
    \item Verification of goal achievement under different payoffs: can we ensure that self-interested agents will, in fact, show cooperative emergent properties? A similar question arises in terms of valuations (how much cost each agent incurs).
    \item A priori consideration of reliability: Can we learn and take into account the fact that robots may fail throughout the progress of a mission when we assign tasks?
\end{itemize}

\subsection*{Stochastic Games}



\section{Hard Coded}

\section{MARL}

Reinforcement learning extends the Markov Decision Process problem by considering the case where the payoff model is not known. This, of course, is the case for most real world environments. As such, MARL algorithms can perhaps be considered to be more applicable than Dec-POMDP models. Fortunately, MARL has picked up a lot of traction in research recently, with a large body dedicated towards solving the many problems it presents.

The largest problem in MARL is the non-stationarity of the environment \cite{Hernandez-LealA}. In single-agent settings, it is assumed that the environment is Markovian. However, this must be lifted in the Multi Agent setting since other agents in the environment will be learning concurrently. This learning will be based on their own history of interactions which extend beyond the previous state. As such, we must now consider that the policy for any one agent will depend on the policy of all other agents. As such, a big concern in this area is regarding convergence guarantees and the stability of the learner system. Approaches to this will be discussed in the next chapter. 

\subsection*{Agent Modelling}

Returning to the problem of non-stationarity, solutions have been presented in which the agent models the learning of other agents. A noteworthy example of this is found in \cite{Foerster2018LearningAwareness} in which the agent performs a one-step lookahead of the other agents' learning and optimises with respect to this expected return. They show that this leads to stable learning and can even lead to emergent cooperation from competition. However, the method requires that both agents have exact knowledge of the others' value functions in order to perform the one step lookahead. Furthermore, it has only been considered for the case of a two agent adversarial game and so the scalability of the system to multiple agents is not yet understood. Another method presented by Mao et al. \cite{MaoModellingDDPG} uses a centralised critic to collect the actions and observations of all agents and allows it to model the joint policy of teammates. This is shown to generate cooperative behaviour across four agents and so is more applicable to real world settings. However, its disadvantage over the method presented in \cite{Foerster2018LearningAwareness} is that the critic is centralised. In real world settings, this requires the presence of an agent (perhaps a laptop) which is able to handle the computational load of determining a joint policy across all agents and must then communicate the Q-values of all agents back to them. This is both a taxing both in terms of computation and time. 

Hong et al \cite{Hong2018ASystems} present a similar system for modelling teammate policies by tasking a CNN with determining the policy features of other agents and then embedding these as features in its own DQN. This shows strong performance in settings where other agents dynamically change their policies. The concerns with this, however, are that, as the number of agents in the field increase, the CNN in each agent must perform another approximation. This places strong requirements on the performance of the CNN since errors in estimation will accumulate as the number of agents increases. Similarly, the complexity of the DQN will increase as more feature vectors are added. 

Finally, all of the above methods are not robust to evolving numbers of agents. The problem of agent modelling is an important one to ensure stable learning and to understand the evolution of the system. It also presents a strong challenge and is open to exploration. To put it in context the methods described in this section are all from 2018-19, so its all very new.

\subsection{Directions}

I still have a lot of reading to do regarding MARL, which, in turn, will identify new directions. However, on initial assessment I put forward

\begin{itemize}
    \item Modelling evolving teammates: The purpose of this is to more strictly ensure the stability of the learning process. However, the particular problem I suggest is to consider the modelling in a decentralised manner and with the consideration of evolving numbers of agents in teams.
\end{itemize}


\section{Dec-POMDPs and MARL}

As Dec-POMDPs are the theoretical formulation of MARL problems, it stands to reason that other methods for solving Dec-POMDPs should provide insights into improving MARL. Fortunately Oliehock in \cite{OliehoekDecentralizedPOMDPs} presents a number of existing methods towards solving Dec-POMDPs. The methods which I feel may be applicable are 
\begin{itemize}
    \item Alternating Maximisation: This is effectively coordinate ascent for determining a joint policy.
    \item Approximation as Bayesian Games: Perhaps solving the repeated Bayesian Game through MARL would be more efficient. It does, however, force us to ask how best to sub-divide a Dec-POMDP into a series of Bayesian Games 
    \item Selecting sub-tree policies: Could DL be used to determine which sub-tree policies are optimal? In order to make this work, we would also need to consider how to select the feature space of sub-trees and how to collect this information.
\end{itemize}

\chapter{Multi Agent Control}

Multi Agent Dynamics considers the problem of mathematically modelling learning in Multi Agent Systems (MAS). This model then serves to be able to predict the evolution of a learning system as well as to understand the trajectory of learning. Typically, this looks at considering whether or not the method is likely to converge towards a Nash equilibrium. This is generally a difficult problem to solve \cite{ShohamMultiagentFoundations} for all but toy problems. To extend this applicability into real world settings requires the study of stable equilbrium points; \cite{Letcher2019DifferentiableMechanics} shows that the stable equilbrium and Nash equilibrium (NE) are not necessarily the same and, in fact, argue that stable points are more informative than NEs. Stability provides some guarantees against the stochastic nature of the environment since a stable equilibrium will always be returned to even after perturbations. This extremely important in Safe and Trusted AI as it provides guarantees against undesired behaviour in real world learning environments. 

\section{Stability of Learning Agents}

Stability may be looked at from the view point of two perspectives. The first is from an optimisation point of view. This considers the dynamics of the learning model, allowing us to better choose our parameters and design our models so that they may converge to a stable result. The second is from the view point of the state-action space of a learnt model. This allows us to determine, before the MAS is deployed, which set of state-action pairs will lead to unstable behaviour. This knowledge allows us to consider which state-action pairs should be avoided. In both cases, stability analysis allows us to build multi agent systems which will learn and act in the way that we expect them to.

In \cite{Letcher2019DifferentiableMechanics}, Letcher et al. model gradient descent learning of generative-adversarial-networks (GANs) as a two-player differentiable game. A differentiable game is one in which the loss function is twice differentiable. Using this formulation, they are able to analyse the system from new perspectives by considering the current state-of-the-art understanding of differentiable game theory. Whilst, at first glance, this may seem like a purely theoretical exercise, they go on to show that the insights gained allow them to develop a new multi-objective optimisation technique for GANs which shows stronger convergence properties, most notably of which is that it guarantees that the method finds a stable equilibrium (and avoids saddles) between the two players' loss functions.

Jin and Lavaei \cite{Jin2018Stability-certifiedPerspective} consider the policy of a reinforcement learning agent as a non-linear, time varying feedback controller. Using this notion, they then consider the bounded-input-bounded-output stability of the system. They do this by analysing the ratio between the total output and total input energy (called the L2 gain). If the L2 gain remains finite then the system may be considered to be stable. They then apply these considerations on real-world applications including multi-agent flight formation and obtain stability certificates (essentially confirming that the system will remain stable under certain conditions) for the learned controller.

Berkenkamp et al. \cite{Berkenkamp2017SafeGuarantees} consider a similar problem from a different definition of stability. Specifically, they look at stability from the point of view of Lyapunov functions. A system is said to be stable if the applying the policy will result in stricly lower evaluations of the function. In other words, a system is stable if its corresponding Lyapunov function is decreasing towards a minimum point. The authors use this idea to define a 'region of attraction' in which the system is stable in the sense of Lyapunov. The goal of Safe Lyapunov Learning, a method which they develop from these insights, is to learn a policy without leaving this region of attraction. They do this by taking measurements within the set and using this to learn about the system dynamics, thereby increasing the safe set. With this, we now have a guarantee that policy optimisation will not result in unsafe behaviour, even in the presence of stochasticity and exploration.

As we can see from the discussion thus far, there are many definitions and perpectives of stability, each lending to a new understanding of learning systems. To add another to the mix, Milchtaich in \cite{Milchtaich2007StaticGames}, presents a notion of static stability. This means that it is based solely on the incentives of the players and does not require a consideration of the dynamics of the system. Milchtaich's definition of a stable system is one in which, when perturbed, it is more beneficial for an agent to move back towards the equilibrium than it is for them to move away. This is perhaps the most fundamental definition of a stable equilibrium and, as such, Milchtaich shows that it is applicable to all strategic games, within certain assumptions (finite set of player and continuous strategy space) and considers probabilistic perturbations from the original state. This is particular applicable to Multi Robot Reinforcement Learning (or any MARL in continuous strategy spaces) since these systems require random exploration of the strategy space and the dynamics are not known a priori. However, the lack of dynamics means that we do not consider the evolution of learning. 


\subsection{Directions}

The above discussion has illustrated a few points. The first, and perhaps most important, is that stability is extremely applicable to Multi Agent Reinforcement Learning. This is because of the fact that learning agents, especially in real world environments, are subject to uncertainty and perturbations as well as epsilon-random exploration. As such, it is important that the system is able to return to a stable state after such perturbations. This leads to the need to develop learning systems as in \cite{Letcher2019DifferentiableMechanics} which are attracted to stable equilibria. Similarly, it allows us to consider existing algorithms and whether they lead to stable equilibria. For instance, Letcher et al show in \cite{LetcherSTABLEGAMES} that the Learning with Opponent Learning Awareness (LOLA) algorithm \cite{Foerster2018LearningAwareness} does not converge to stable fixed points.

The second point is that the notion of stability has multiple definitions. Each one is appropriate in different conditions and, as such, it is important to explore stability from various viewpoints to consider how to best understand the evolution of a learning system. 

Another factor to consider is suggested in \cite{Milchtaich2007StaticGames}; multiplayer games do not necessarily contain stable fixed points. The existence of stable equilibria is therefore an open question and is well worth consideration. This would help guide the choice of loss functions and strategy spaces. This is likely to be an extremely difficult problem to generalise but would have a dramatic impact on the safety certification of MARL systems.

The problem of finding stable solutions through multi agent dynamics also opens the door for a large degree of application domains which are currently quite sparse. I would recommend looking at the contents page of \cite{Hamalainen1991DifferentialFinland} which illustrates a number of applications of dynamic and differential game theory in a number of domains including: pursuit-evasion, systems control and economic modelling. My particular interest, as always, lies within robotic control problems. This appears to be the most direct and relevant application of stable MAS systems; we can, for instance, use MARL to determine control rules which will lead to stable performance of the system. An example of this might be the problem of Cooperative Moving Path Following Control considered by \cite{Reis2019RobustVehicles}. Here, multiple agents must track moving targets according to a pre-defined path without interfering with one another. It is clear then that each player's control is dependent on the other and so can be modelled as a game theoretic problem. Our task, then, is to determine control rules which lead to stable coordination across the team. Similarly, we can model consider modelling a controller and an adversarial environment as a zero-sum game as examined in \cite{Marden2018AnnualControl}. Another application is proposed by Letcher et al in \cite{Letcher2019DifferentiableMechanics} - that of Generative adversarial networks (GANs) which can be considered as a two player minimax game with differentiable loss functions. 

The following papers \cite{Bailey2019FiniteDescent-Ascent, Bailey2019Multi-AgentSystem, Boone2019FromTheory, DickensTheLearning, Berkenkamp2017SafeGuarantees, Jin2018Stability-certifiedPerspective, Letcher2019DifferentiableMechanics} are the ones that I found particularly relevant to this study. However, they will require some further analysis before I write about them here.


\bibliographystyle{IEEEtran}
\bibliography{IEEEfull,references}

\end{document} 
