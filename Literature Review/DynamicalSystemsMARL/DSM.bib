@article{Sanders2018,
abstract = {We study adaptive learning in a typical p-player game. The payoffs of the games are randomly generated and then held fixed. The strategies of the players evolve through time as the players learn. The trajectories in the strategy space display a range of qualitatively different behaviours, with attractors that include unique fixed points, multiple fixed points, limit cycles and chaos. In the limit where the game is complicated, in the sense that the players can take many possible actions, we use a generating-functional approach to establish the parameter range in which learning dynamics converge to a stable fixed point. The size of this region goes to zero as the number of players goes to infinity, suggesting that complex non-equilibrium behaviour, exemplified by chaos, is the norm for complicated games with many players.},
archivePrefix = {arXiv},
arxivId = {1612.08111},
author = {Sanders, James B.T. and Farmer, J. Doyne and Galla, Tobias},
doi = {10.1038/s41598-018-22013-5},
eprint = {1612.08111},
file = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanders, Farmer, Galla - 2018 - The prevalence of chaotic dynamics in games with many players.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
number = {1},
publisher = {Nature Publishing Group},
title = {{The prevalence of chaotic dynamics in games with many players}},
volume = {8},
year = {2018}
}
@techreport{Hu,
abstract = {Modelling the dynamics of multi-agent learning has long been an important research topic, but all of the previous works focus on 2-agent settings and mostly use evolutionary game theoretic approaches. In this paper, we study an n-agent setting with n tends to infinity, such that agents learn their policies concurrently over repeated symmetric bimatrix games with some other agents. Using the mean field theory, we approximate the effects of other agents on a single agent by an averaged effect. A Fokker-Planck equation that describes the evolution of the probability distribution of Q-values in the agent population is derived. To the best of our knowledge, this is the first time to show the Q-learning dynamics under an n-agent setting can be described by a system of only three equations. We validate our model through comparisons with agent-based simulations on typical symmetric bimatrix games and different initial settings of Q-values.},
author = {Hu, Shuyue and Leung, Chin-Wing and Leung, Ho-Fung},
file = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu, Leung, Leung - Unknown - Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games a Mean Field Theoretic Approach.pdf:pdf},
title = {{Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games: a Mean Field Theoretic Approach}}
}
@misc{Tuyls2006,
abstract = {In this paper, we investigate Reinforcement learning (RL) in multi-agent systems (MAS) from an evolutionary dynamical perspective. Typical for a MAS is that the environment is not stationary and the Markov property is not valid. This requires agents to be adaptive. RL is a natural approach to model the learning of individual agents. These Learning algorithms are however known to be sensitive to the correct choice of parameter settings for single agent systems. This issue is more prevalent in the MAS case due to the changing interactions amongst the agents. It is largely an open question for a developer of MAS of how to design the individual agents such that, through learning, the agents as a collective arrive at good solutions. We will show that modeling RL in MAS, by taking an evolutionary game theoretic point of view, is a new and potentially successful way to guide learning agents to the most suitable solution for their task at hand. We show how evolutionary dynamics (ED) from Evolutionary Game Theory can help the developer of a MAS in good choices of parameter settings of the used RL algorithms. The ED essentially predict the equilibriums outcomes of the MAS where the agents use individual RL algorithms. More specifically, we show how the ED predict the learning trajectories of Q-Learners for iterated games. Moreover, we apply our results to (an extension of) the COllective INtelligence framework (COIN). COIN is a proved engineering approach for learning of cooperative tasks in MASs. The utilities of the agents are re-engineered to contribute to the global utility. We show how the improved results for MAS RL in COIN, and a developed extension, are predicted by the ED. {\textcopyright} 2005 Springer Science+Business Media, Inc.},
author = {Tuyls, Karl and {T Hoen}, Pieter Jan and Vanschoenwinkel, Bram},
booktitle = {Autonomous Agents and Multi-Agent Systems},
doi = {10.1007/s10458-005-3783-9},
file = {:home/aamalh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuyls, T Hoen, Vanschoenwinkel - 2006 - An evolutionary dynamical analysis of multi-agent learning in iterated games.pdf:pdf},
issn = {13872532},
keywords = {COllective INtelligence,Evolutionary Game Theory,Iterated games,Multi-agent systems,Reinforcement learning},
month = {jan},
number = {1},
pages = {115--153},
title = {{An evolutionary dynamical analysis of multi-agent learning in iterated games}},
volume = {12},
year = {2006}
}
