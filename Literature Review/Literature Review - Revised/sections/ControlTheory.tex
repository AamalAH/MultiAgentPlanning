\documentclass[../sample.tex]{subfiles}

\begin{document}

The control theoretic perspective considers generating a set of control laws for the system. These
are chosen with the aim to satisfy certain properties. The main properties are

\begin{itemize}
	\item Stability, that a system will return to the desired setpoint (or within a neighbourhood) if
	perturbed.

	\item Robustness, that a system will perform its function in the presence of uncertainty and
	noise

	\item Optimality, that the system will achieve some defined function 

	\item Feasibility, that the controller will always be able to generate a control law which
	satisfies the desired properties.
\end{itemize}


There are a number of approaches towards control systems. However, the interest of this review lies
in multi-agent systems which must operate in the face of uncertainty. As such, we focus on
stochastic and distributed control. The particular methodology that this chapter focuses largely on
is Model Predictive Control (MPC) as STAI is particularly interested in model-based techniques for
autonomous systems.

\section{Model Predictive Control}

Model Predictive Control is a long standing paradigm in AI which looks specifically at the problem
of operating real-world agents safely in the face of environmental disturbances. The overarching
idea begins with the assumpion that we have a model of the environment. As an example, in
the case of autonomous vehicles, we have a model of how adjusting the angle of the front wheels will
affect the heading of the car. We then perform a finite horizon look ahead, in which we estimate the
environment state for a few time steps ahead, and generate a policy for this horizon. The agent
performs the immediate action generated by the policy, and then we repeat the process, after taking
measurements of the environment to determine the error in the system. Throughout this process, the
controller (a.k.a. policy) must remain stable, but also satisfy constraints. From our previous
example, a likely constraint would be that the controller will never result in the car going on the
pavement, where it would present a real hazard to pedestrians. It is through these constraints, and
the requirement of stability that the system is required to remain safe throughout operation. To
that end, mathematical proofs of these properties are provided in the literature ensuring that we
can trust in the system's performance.

Deterministic MPC assumes that the environment is completely deterministic and, therefore, assumes
knowledge of the exact nature of the disturbances. This somewhat naive assumption simplifies
computation and so appears often in the literature, as in \cite{Rosolia2018}. However, it does not
provide strong guarantees outside of games and simulations. Robust MPC brings this to a higher level
of abstraction in which system perturbations, though deterministic, lie on a bounded set. Therefore,
we no longer assume the exact nature of the environment and can guard against worst case scenarios.
This is extremely effective in closed environments or for simple tasks, but falters in more complex
environments.

Stochastic Model Predictive Control (SMPC) lifts the assumption made in MPC, namely that
perturbations are deterministic and lie on a bounded set \cite{Mesbah2016}. To accomplish this, we
define chance constraints, for which probabilistic guarantees must be determined. Furthermore,
optimality is defined in terms of the minimisation of the expectation of a probabilistic
cost function. This formulation brings about controllers which are more applicable and robust when
deployed. However, the field is in its infancy and presents a number of theoretical challenges.
These are best described by Mesbah in \cite{Mesbah2016}

\begin{enumerate}
	\item The arbitrary form of the feedback control laws
	\item The non-convexity and intractability of the chance constraints
	\item The complexity of the uncertainty propagations
	\item Establishing stability of the control problem
\end{enumerate}

Since the review \cite{Mesbah2016} was written in 2016, there have been a number of approaches
towards solving some of these problems, most notably the intractability of the chance contraints.
For instance, in  \cite{Paulson2019}, Paulson and Mesbah propose the use of joint chance constraints
in considering time varying stochastic disturbances as well as model uncertainty. This is shown to
be strongly suited to non-linear systems, which itself is an open problem in the MPC sphere.

\subsection{Distributed MPC}

Rawlings et al \cite{rawlings2017model} divide the problem of distributed control into four distinct
categories: decentralised control, non-cooperative control, cooperative control, and centralised
control. The last of these is not considered in the text since it only considers the case in which a
centralised controller has access and can manipulate multiple agents at once. 

Decentralised control is the scheme in which agents do not have information about the actions of
other agents and can only optimise for their own objective. This has the advantage of requiring no
communication, but can often lead to poor performance when the agents are strongly coupled as each
agent’s model is incomplete. In the non-cooperative setting, each agent optimises their own loss
function whilst treating the others’ actions as a known disturbance. In this setting, each agent has
knowledge of the others’ control laws and their effect. In this case, the agents must communicate
their intended actions to one another and iterate to achieve a consensus (or Nash equilibrium). This
is the same for cooperative control, except that the loss function is now shared across the team. 

Of all of these systems, cooperative control has found greatest applicability in autonomous systems
\cite{Negenborn2014}, perhaps due to the favourable stability properties \cite{Venkat2006}. One
particularly strong application of this system is in vehicle platooning. Here, self-driving cars or
unmanned aerial vehicles (UAVs) must move in a certain formation without colliding into one another.
A number of examples can be found such as in \cite{Liu2019, VanParys2017, Zheng2017}. Distributed
MPC provides the advantage that guarantees can be placed regarding coupled constraint satisfaction and
feasibility.

The advances in stochastic and robust MPC, however, do lend themselves towards revisiting the
capabilities of decentralised control. Recall that, in this scheme, the agents cannot communicate
with one another. However, advances in MARL show us that this disadvantage can be made less
prevalent if each agent has a model of the other \cite{Foerster}. To this end, a methodology such as
presented in \cite{Heirung2019} may prove beneficial in the decentralised scheme. Here, the system
chooses amongst a family of system models when choosing its control laws. Similarly, agents who
determine (or perhaps learn) models of the other agents may be able to leverage this information,
minimising the model error when optimising.

\subsection{Hybrid Control}

Hybrid control considers agents whose behaviour is governed not only by continuous time dynamics but
also according to discrete switches \cite{Lygeros2004}. This is particularly useful in systems
which may have different modes of operations (such as autonomous vehicles with gear shifts or
mechanical systems which undergo collisions). It is particularly useful for the case of systems
which undergo discrete failures (e.g. burst tire in an self-driving car). It is important,
therefore, that control systems be designed with the aim to account for such switching behaviour. 

Mendes et al \cite{Mendes2017} present one of the first practical implementations of MPC for
distributed hybrid systems. Here, a Mixed Integer Quadratic Program (MIQP) is transformed into a set
of QPs by generating a controller for each of the feasible combinations of binary variables. An
iterative search is then used to determine the best solution out of this family of distributed
control laws. The method is shown to reduce optimality slightly in favour of efficient computation.
It would perhaps be interesting to consider distributed control from the perspective of fault
detection and recovery, and consider how a system similar to that of Tarapore et al may be used in
place of generating all possible controllers. Since Tarapore’s method is developed for swarms, it
maintains the advantages of low communication overheads but may prove advantageous in terms of
communication.

Mendes’ method, however, considers deterministic switching control. When considering faults,
however, this can no longer be the case, since faults occur stochastically. To this end, it is
important to consider stochastic switching dynamics. One proposed method is that of Jump Markovian
Linear Systems (JMLS). In this, discrete ‘jumps’ can occur in a system which, otherwise is governed
by continuous time dynamics \cite{Ma2017}. To this end, Blackmore et al \cite{Blackmore2007} develop
a method towards predictive control of such systems using a particle filtering approach. The aim of
this method is for the resultant system to remain within a safe set with a defined maximum
probability which takes into account the discrete changes (which they consider as brake failures in
an autonomous vehicle), as well as the usual continuous stochasticity which occurs due to noise and
uncertainty. Their method is seen to remain robust to both of these and allows for safe operation
(by constraint satisfaction) of an autonomous vehicle. Yin et al \cite{Yin2014}, similarly
considering this problem, but for the specific case in which the transition probability of the MJLS
is partially unknown. To this end, they similarly develop a controller which is independent of the
mode of operation (and so can operate safely in any of those modes) but supplement it with a
mode-dependent controller which drives the system initially. This allows for a better implementation
than in \cite{Blackmore2007} in terms of optimality, but at the expense of robustness and
computation. 

Stochastic Hybrid systems have yet to find their way into distributed predictive control settings
but may prove critical for developing multi-agent systems which are robust to agent failures.
Consider, for instance, the case of the platooning problem but with the added complexity that agents
within the platoon may undergo failures in their braking system. Similarly, communication failures
may occur when driving which may result in an agent having to operate with reduced communication
capacity. Developing Stochastic Hybrid Distributed Systems may present the opportunity for
autonomous systems to remain robust to known failures. 


\end{document}